{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7697424f-b391-4b01-aca0-0da917adfb65",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766e2b5b-017b-4935-aed0-d94f9fb5c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de982936-c988-4ff1-b120-d650cc72acea",
   "metadata": {},
   "source": [
    "# To read images/ prepare feature vector and convert labels to one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a07f925-eef7-439c-a3a1-f1786b749fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Array Shape: (42000, 784)\n",
      "One-Hot Labels Shape: (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_labels(labels):\n",
    "    \"\"\"\n",
    "    Convert labels into one-hot encoded vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        labels: List or 1D array of integers (0-9).\n",
    "\n",
    "    Returns:\n",
    "        one_hot_labels: 2D NumPy array of shape (n_samples, 10).\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((n_samples, 10))\n",
    "    one_hot_labels[np.arange(n_samples), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Extract features from a 28x28 image by flattening it into a 1D array.\n",
    "    \n",
    "    Parameters:\n",
    "        image: 2D NumPy array (28x28) - grayscale image.\n",
    "\n",
    "    Returns:\n",
    "        features: 1D NumPy array (784) - flattened image.\n",
    "    \"\"\"\n",
    "    return image.flatten()\n",
    "\n",
    "\n",
    "def preprocess_dataset(image_folder_path):\n",
    "    \"\"\"\n",
    "    Process images and labels for a digit recognition model.\n",
    "\n",
    "    Parameters:\n",
    "        image_folder_path: Path to the dataset folder containing subfolders 0-9.\n",
    "\n",
    "    Returns:\n",
    "        features_array: 2D NumPy array (n_samples, 784) - flattened image features.\n",
    "        one_hot_labels: 2D NumPy array (n_samples, 10) - one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Traverse folders (0-9)\n",
    "    for label in range(10):\n",
    "        label_folder = os.path.join(image_folder_path, str(label))\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Open and process the image\n",
    "            image = Image.open(file_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "            image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "            features = extract_features(image)\n",
    "\n",
    "            # Append features and label\n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    features_array = np.array(features_list)\n",
    "    one_hot_labels = one_hot_encode_labels(labels_list)\n",
    "\n",
    "    return features_array, one_hot_labels\n",
    "\n",
    "\n",
    "image_folder_path = r\"C:\\Users\\vijay\\Documents\\digit_rec\\trainingSet\\trainingSet\"  # Replace with your dataset path\n",
    "features_array, one_hot_labels = preprocess_dataset(image_folder_path)\n",
    "\n",
    "print(\"Feature Array Shape:\", features_array.shape)  # Should be (n_samples, 784)\n",
    "print(\"One-Hot Labels Shape:\", one_hot_labels.shape)  # Should be (n_samples, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0a0fd-cfe7-4d70-bd3f-83f02084eea8",
   "metadata": {},
   "source": [
    "# ReLu and it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e517480c-330e-4449-986e-5f5f6df9c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0ad7e-fd35-4572-9068-d93ecd9ebc9b",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd3f9d76-5100-49aa-adab-d12a45f968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913288f-90eb-459e-bdb9-f684a14922c5",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "958b70a4-6aad-4b8c-b51c-761f6caf57b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Categorical cross-entropy loss with epsilon to avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8c378-84ca-4064-aa05-16397a02138d",
   "metadata": {},
   "source": [
    "# Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd7c1744-de58-4583-8fe9-e5d75d7deed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 0.6598, Accuracy: 0.8030\n",
      "Validation Loss: 0.3671, Validation Accuracy: 0.8874\n",
      "0.001 \n",
      "\n",
      "Epoch 2/100 - Loss: 0.2855, Accuracy: 0.9168\n",
      "Validation Loss: 0.2408, Validation Accuracy: 0.9310\n",
      "0.00095 \n",
      "\n",
      "Epoch 3/100 - Loss: 0.2028, Accuracy: 0.9413\n",
      "Validation Loss: 0.1895, Validation Accuracy: 0.9475\n",
      "0.0009025 \n",
      "\n",
      "Epoch 4/100 - Loss: 0.1633, Accuracy: 0.9538\n",
      "Validation Loss: 0.1702, Validation Accuracy: 0.9532\n",
      "0.000857375 \n",
      "\n",
      "Epoch 5/100 - Loss: 0.1413, Accuracy: 0.9611\n",
      "Validation Loss: 0.1603, Validation Accuracy: 0.9574\n",
      "0.0008145062499999999 \n",
      "\n",
      "Epoch 6/100 - Loss: 0.1273, Accuracy: 0.9660\n",
      "Validation Loss: 0.1542, Validation Accuracy: 0.9618\n",
      "0.0007737809374999998 \n",
      "\n",
      "Epoch 7/100 - Loss: 0.1172, Accuracy: 0.9694\n",
      "Validation Loss: 0.1508, Validation Accuracy: 0.9631\n",
      "0.0007350918906249999 \n",
      "\n",
      "Epoch 8/100 - Loss: 0.1095, Accuracy: 0.9718\n",
      "Validation Loss: 0.1487, Validation Accuracy: 0.9640\n",
      "0.0006983372960937497 \n",
      "\n",
      "Epoch 9/100 - Loss: 0.1039, Accuracy: 0.9738\n",
      "Validation Loss: 0.1481, Validation Accuracy: 0.9652\n",
      "0.0006634204312890623 \n",
      "\n",
      "Epoch 10/100 - Loss: 0.0991, Accuracy: 0.9763\n",
      "Validation Loss: 0.1469, Validation Accuracy: 0.9655\n",
      "0.0006302494097246091 \n",
      "\n",
      "Epoch 11/100 - Loss: 0.0951, Accuracy: 0.9779\n",
      "Validation Loss: 0.1478, Validation Accuracy: 0.9650\n",
      "0.0005987369392383787 \n",
      "\n",
      "Epoch 12/100 - Loss: 0.0919, Accuracy: 0.9793\n",
      "Validation Loss: 0.1490, Validation Accuracy: 0.9655\n",
      "0.0005688000922764596 \n",
      "\n",
      "Epoch 13/100 - Loss: 0.0893, Accuracy: 0.9805\n",
      "Validation Loss: 0.1515, Validation Accuracy: 0.9649\n",
      "0.0005403600876626366 \n",
      "\n",
      "Epoch 14/100 - Loss: 0.0868, Accuracy: 0.9816\n",
      "Validation Loss: 0.1546, Validation Accuracy: 0.9663\n",
      "0.0005133420832795048 \n",
      "\n",
      "Epoch 15/100 - Loss: 0.0849, Accuracy: 0.9822\n",
      "Validation Loss: 0.1575, Validation Accuracy: 0.9661\n",
      "0.00048767497911552955 \n",
      "\n",
      "Epoch 16/100 - Loss: 0.0832, Accuracy: 0.9831\n",
      "Validation Loss: 0.1600, Validation Accuracy: 0.9662\n",
      "0.000463291230159753 \n",
      "\n",
      "Epoch 17/100 - Loss: 0.0815, Accuracy: 0.9834\n",
      "Validation Loss: 0.1638, Validation Accuracy: 0.9662\n",
      "0.00044012666865176535 \n",
      "\n",
      "Epoch 18/100 - Loss: 0.0800, Accuracy: 0.9840\n",
      "Validation Loss: 0.1669, Validation Accuracy: 0.9658\n",
      "0.0004181203352191771 \n",
      "\n",
      "Epoch 19/100 - Loss: 0.0789, Accuracy: 0.9843\n",
      "Validation Loss: 0.1699, Validation Accuracy: 0.9661\n",
      "0.0003972143184582182 \n",
      "\n",
      "Epoch 20/100 - Loss: 0.0778, Accuracy: 0.9849\n",
      "Validation Loss: 0.1726, Validation Accuracy: 0.9665\n",
      "0.00037735360253530727 \n",
      "\n",
      "Epoch 21/100 - Loss: 0.0768, Accuracy: 0.9852\n",
      "Validation Loss: 0.1756, Validation Accuracy: 0.9671\n",
      "0.0003584859224085419 \n",
      "\n",
      "Epoch 22/100 - Loss: 0.0757, Accuracy: 0.9857\n",
      "Validation Loss: 0.1784, Validation Accuracy: 0.9670\n",
      "0.0003405616262881148 \n",
      "\n",
      "Epoch 23/100 - Loss: 0.0747, Accuracy: 0.9860\n",
      "Validation Loss: 0.1810, Validation Accuracy: 0.9675\n",
      "0.000323533544973709 \n",
      "\n",
      "Epoch 24/100 - Loss: 0.0735, Accuracy: 0.9864\n",
      "Validation Loss: 0.1839, Validation Accuracy: 0.9675\n",
      "0.00030735686772502356 \n",
      "\n",
      "Epoch 25/100 - Loss: 0.0723, Accuracy: 0.9871\n",
      "Validation Loss: 0.1859, Validation Accuracy: 0.9673\n",
      "0.0002919890243387724 \n",
      "\n",
      "Epoch 26/100 - Loss: 0.0713, Accuracy: 0.9875\n",
      "Validation Loss: 0.1882, Validation Accuracy: 0.9671\n",
      "0.00027738957312183375 \n",
      "\n",
      "Epoch 27/100 - Loss: 0.0703, Accuracy: 0.9878\n",
      "Validation Loss: 0.1903, Validation Accuracy: 0.9674\n",
      "0.00026352009446574203 \n",
      "\n",
      "Epoch 28/100 - Loss: 0.0694, Accuracy: 0.9881\n",
      "Validation Loss: 0.1925, Validation Accuracy: 0.9671\n",
      "0.00025034408974245495 \n",
      "\n",
      "Epoch 29/100 - Loss: 0.0687, Accuracy: 0.9883\n",
      "Validation Loss: 0.1946, Validation Accuracy: 0.9670\n",
      "0.00023782688525533216 \n",
      "\n",
      "Epoch 30/100 - Loss: 0.0679, Accuracy: 0.9885\n",
      "Validation Loss: 0.1967, Validation Accuracy: 0.9669\n",
      "0.00022593554099256555 \n",
      "\n",
      "Epoch 31/100 - Loss: 0.0672, Accuracy: 0.9886\n",
      "Validation Loss: 0.1986, Validation Accuracy: 0.9667\n",
      "0.00021463876394293727 \n",
      "\n",
      "Epoch 32/100 - Loss: 0.0664, Accuracy: 0.9888\n",
      "Validation Loss: 0.2002, Validation Accuracy: 0.9667\n",
      "0.00020390682574579038 \n",
      "\n",
      "Epoch 33/100 - Loss: 0.0657, Accuracy: 0.9889\n",
      "Validation Loss: 0.2019, Validation Accuracy: 0.9665\n",
      "0.00019371148445850088 \n",
      "\n",
      "Epoch 34/100 - Loss: 0.0650, Accuracy: 0.9891\n",
      "Validation Loss: 0.2035, Validation Accuracy: 0.9663\n",
      "0.00018402591023557584 \n",
      "\n",
      "Epoch 35/100 - Loss: 0.0642, Accuracy: 0.9890\n",
      "Validation Loss: 0.2045, Validation Accuracy: 0.9661\n",
      "0.000174824614723797 \n",
      "\n",
      "Epoch 36/100 - Loss: 0.0634, Accuracy: 0.9891\n",
      "Validation Loss: 0.2063, Validation Accuracy: 0.9662\n",
      "0.00016608338398760718 \n",
      "\n",
      "Epoch 37/100 - Loss: 0.0627, Accuracy: 0.9892\n",
      "Validation Loss: 0.2078, Validation Accuracy: 0.9662\n",
      "0.0001577792147882268 \n",
      "\n",
      "Epoch 38/100 - Loss: 0.0621, Accuracy: 0.9893\n",
      "Validation Loss: 0.2090, Validation Accuracy: 0.9662\n",
      "0.00014989025404881545 \n",
      "\n",
      "Epoch 39/100 - Loss: 0.0615, Accuracy: 0.9896\n",
      "Validation Loss: 0.2104, Validation Accuracy: 0.9662\n",
      "0.00014239574134637466 \n",
      "\n",
      "Epoch 40/100 - Loss: 0.0611, Accuracy: 0.9898\n",
      "Validation Loss: 0.2115, Validation Accuracy: 0.9663\n",
      "0.00013527595427905592 \n",
      "\n",
      "Epoch 41/100 - Loss: 0.0606, Accuracy: 0.9900\n",
      "Validation Loss: 0.2124, Validation Accuracy: 0.9662\n",
      "0.00012851215656510312 \n",
      "\n",
      "Epoch 42/100 - Loss: 0.0601, Accuracy: 0.9900\n",
      "Validation Loss: 0.2133, Validation Accuracy: 0.9667\n",
      "0.00012208654873684796 \n",
      "\n",
      "Epoch 43/100 - Loss: 0.0596, Accuracy: 0.9902\n",
      "Validation Loss: 0.2142, Validation Accuracy: 0.9668\n",
      "0.00011598222130000556 \n",
      "\n",
      "Epoch 44/100 - Loss: 0.0590, Accuracy: 0.9902\n",
      "Validation Loss: 0.2153, Validation Accuracy: 0.9667\n",
      "0.00011018311023500529 \n",
      "\n",
      "Epoch 45/100 - Loss: 0.0586, Accuracy: 0.9906\n",
      "Validation Loss: 0.2163, Validation Accuracy: 0.9663\n",
      "0.00010467395472325501 \n",
      "\n",
      "Epoch 46/100 - Loss: 0.0581, Accuracy: 0.9907\n",
      "Validation Loss: 0.2174, Validation Accuracy: 0.9663\n",
      "9.944025698709225e-05 \n",
      "\n",
      "Epoch 47/100 - Loss: 0.0577, Accuracy: 0.9908\n",
      "Validation Loss: 0.2185, Validation Accuracy: 0.9663\n",
      "9.446824413773763e-05 \n",
      "\n",
      "Epoch 48/100 - Loss: 0.0572, Accuracy: 0.9909\n",
      "Validation Loss: 0.2192, Validation Accuracy: 0.9661\n",
      "8.974483193085076e-05 \n",
      "\n",
      "Epoch 49/100 - Loss: 0.0568, Accuracy: 0.9909\n",
      "Validation Loss: 0.2202, Validation Accuracy: 0.9660\n",
      "8.52575903343082e-05 \n",
      "\n",
      "Epoch 50/100 - Loss: 0.0564, Accuracy: 0.9909\n",
      "Validation Loss: 0.2209, Validation Accuracy: 0.9658\n",
      "8.099471081759279e-05 \n",
      "\n",
      "Epoch 51/100 - Loss: 0.0560, Accuracy: 0.9909\n",
      "Validation Loss: 0.2218, Validation Accuracy: 0.9657\n",
      "7.694497527671315e-05 \n",
      "\n",
      "Epoch 52/100 - Loss: 0.0556, Accuracy: 0.9909\n",
      "Validation Loss: 0.2226, Validation Accuracy: 0.9655\n",
      "7.30977265128775e-05 \n",
      "\n",
      "Epoch 53/100 - Loss: 0.0553, Accuracy: 0.9910\n",
      "Validation Loss: 0.2230, Validation Accuracy: 0.9656\n",
      "6.94428401872336e-05 \n",
      "\n",
      "Epoch 54/100 - Loss: 0.0550, Accuracy: 0.9910\n",
      "Validation Loss: 0.2234, Validation Accuracy: 0.9657\n",
      "6.597069817787194e-05 \n",
      "\n",
      "Epoch 55/100 - Loss: 0.0547, Accuracy: 0.9909\n",
      "Validation Loss: 0.2241, Validation Accuracy: 0.9661\n",
      "6.267216326897833e-05 \n",
      "\n",
      "Epoch 56/100 - Loss: 0.0544, Accuracy: 0.9910\n",
      "Validation Loss: 0.2246, Validation Accuracy: 0.9657\n",
      "5.953855510552941e-05 \n",
      "\n",
      "Epoch 57/100 - Loss: 0.0542, Accuracy: 0.9910\n",
      "Validation Loss: 0.2249, Validation Accuracy: 0.9657\n",
      "5.656162735025293e-05 \n",
      "\n",
      "Epoch 58/100 - Loss: 0.0539, Accuracy: 0.9911\n",
      "Validation Loss: 0.2254, Validation Accuracy: 0.9660\n",
      "5.373354598274029e-05 \n",
      "\n",
      "Epoch 59/100 - Loss: 0.0536, Accuracy: 0.9911\n",
      "Validation Loss: 0.2259, Validation Accuracy: 0.9657\n",
      "5.1046868683603266e-05 \n",
      "\n",
      "Epoch 60/100 - Loss: 0.0533, Accuracy: 0.9911\n",
      "Validation Loss: 0.2265, Validation Accuracy: 0.9657\n",
      "4.8494525249423105e-05 \n",
      "\n",
      "Epoch 61/100 - Loss: 0.0530, Accuracy: 0.9912\n",
      "Validation Loss: 0.2269, Validation Accuracy: 0.9656\n",
      "4.6069798986951947e-05 \n",
      "\n",
      "Epoch 62/100 - Loss: 0.0528, Accuracy: 0.9914\n",
      "Validation Loss: 0.2275, Validation Accuracy: 0.9656\n",
      "4.3766309037604346e-05 \n",
      "\n",
      "Epoch 63/100 - Loss: 0.0526, Accuracy: 0.9914\n",
      "Validation Loss: 0.2279, Validation Accuracy: 0.9655\n",
      "4.157799358572413e-05 \n",
      "\n",
      "Epoch 64/100 - Loss: 0.0524, Accuracy: 0.9914\n",
      "Validation Loss: 0.2284, Validation Accuracy: 0.9657\n",
      "3.949909390643792e-05 \n",
      "\n",
      "Epoch 65/100 - Loss: 0.0521, Accuracy: 0.9914\n",
      "Validation Loss: 0.2288, Validation Accuracy: 0.9657\n",
      "3.752413921111602e-05 \n",
      "\n",
      "Epoch 66/100 - Loss: 0.0519, Accuracy: 0.9914\n",
      "Validation Loss: 0.2293, Validation Accuracy: 0.9657\n",
      "3.564793225056022e-05 \n",
      "\n",
      "Epoch 67/100 - Loss: 0.0517, Accuracy: 0.9915\n",
      "Validation Loss: 0.2295, Validation Accuracy: 0.9657\n",
      "3.386553563803221e-05 \n",
      "\n",
      "Epoch 68/100 - Loss: 0.0515, Accuracy: 0.9915\n",
      "Validation Loss: 0.2299, Validation Accuracy: 0.9656\n",
      "3.217225885613059e-05 \n",
      "\n",
      "Epoch 69/100 - Loss: 0.0513, Accuracy: 0.9915\n",
      "Validation Loss: 0.2303, Validation Accuracy: 0.9658\n",
      "3.0563645913324066e-05 \n",
      "\n",
      "Epoch 70/100 - Loss: 0.0511, Accuracy: 0.9916\n",
      "Validation Loss: 0.2307, Validation Accuracy: 0.9660\n",
      "2.903546361765786e-05 \n",
      "\n",
      "Epoch 71/100 - Loss: 0.0510, Accuracy: 0.9917\n",
      "Validation Loss: 0.2310, Validation Accuracy: 0.9657\n",
      "2.7583690436774966e-05 \n",
      "\n",
      "Epoch 72/100 - Loss: 0.0508, Accuracy: 0.9918\n",
      "Validation Loss: 0.2314, Validation Accuracy: 0.9656\n",
      "2.6204505914936218e-05 \n",
      "\n",
      "Epoch 73/100 - Loss: 0.0506, Accuracy: 0.9918\n",
      "Validation Loss: 0.2317, Validation Accuracy: 0.9655\n",
      "2.4894280619189404e-05 \n",
      "\n",
      "Epoch 74/100 - Loss: 0.0504, Accuracy: 0.9919\n",
      "Validation Loss: 0.2318, Validation Accuracy: 0.9655\n",
      "2.3649566588229933e-05 \n",
      "\n",
      "Epoch 75/100 - Loss: 0.0503, Accuracy: 0.9919\n",
      "Validation Loss: 0.2320, Validation Accuracy: 0.9655\n",
      "2.2467088258818436e-05 \n",
      "\n",
      "Epoch 76/100 - Loss: 0.0501, Accuracy: 0.9920\n",
      "Validation Loss: 0.2322, Validation Accuracy: 0.9655\n",
      "2.134373384587751e-05 \n",
      "\n",
      "Epoch 77/100 - Loss: 0.0500, Accuracy: 0.9920\n",
      "Validation Loss: 0.2325, Validation Accuracy: 0.9655\n",
      "2.0276547153583635e-05 \n",
      "\n",
      "Epoch 78/100 - Loss: 0.0499, Accuracy: 0.9920\n",
      "Validation Loss: 0.2327, Validation Accuracy: 0.9656\n",
      "1.9262719795904453e-05 \n",
      "\n",
      "Epoch 79/100 - Loss: 0.0498, Accuracy: 0.9921\n",
      "Validation Loss: 0.2329, Validation Accuracy: 0.9656\n",
      "1.8299583806109228e-05 \n",
      "\n",
      "Epoch 80/100 - Loss: 0.0496, Accuracy: 0.9921\n",
      "Validation Loss: 0.2331, Validation Accuracy: 0.9654\n",
      "1.7384604615803768e-05 \n",
      "\n",
      "Epoch 81/100 - Loss: 0.0495, Accuracy: 0.9922\n",
      "Validation Loss: 0.2332, Validation Accuracy: 0.9655\n",
      "1.651537438501358e-05 \n",
      "\n",
      "Epoch 82/100 - Loss: 0.0494, Accuracy: 0.9922\n",
      "Validation Loss: 0.2333, Validation Accuracy: 0.9654\n",
      "1.56896056657629e-05 \n",
      "\n",
      "Epoch 83/100 - Loss: 0.0493, Accuracy: 0.9922\n",
      "Validation Loss: 0.2335, Validation Accuracy: 0.9654\n",
      "1.4905125382474753e-05 \n",
      "\n",
      "Epoch 84/100 - Loss: 0.0492, Accuracy: 0.9922\n",
      "Validation Loss: 0.2336, Validation Accuracy: 0.9656\n",
      "1.4159869113351015e-05 \n",
      "\n",
      "Epoch 85/100 - Loss: 0.0491, Accuracy: 0.9922\n",
      "Validation Loss: 0.2337, Validation Accuracy: 0.9655\n",
      "1.3451875657683464e-05 \n",
      "\n",
      "Epoch 86/100 - Loss: 0.0490, Accuracy: 0.9922\n",
      "Validation Loss: 0.2338, Validation Accuracy: 0.9654\n",
      "1.2779281874799288e-05 \n",
      "\n",
      "Epoch 87/100 - Loss: 0.0489, Accuracy: 0.9922\n",
      "Validation Loss: 0.2340, Validation Accuracy: 0.9654\n",
      "1.2140317781059324e-05 \n",
      "\n",
      "Epoch 88/100 - Loss: 0.0488, Accuracy: 0.9922\n",
      "Validation Loss: 0.2341, Validation Accuracy: 0.9652\n",
      "1.1533301892006358e-05 \n",
      "\n",
      "Epoch 89/100 - Loss: 0.0487, Accuracy: 0.9922\n",
      "Validation Loss: 0.2342, Validation Accuracy: 0.9652\n",
      "1.095663679740604e-05 \n",
      "\n",
      "Epoch 90/100 - Loss: 0.0487, Accuracy: 0.9922\n",
      "Validation Loss: 0.2343, Validation Accuracy: 0.9654\n",
      "1.0408804957535738e-05 \n",
      "\n",
      "Epoch 91/100 - Loss: 0.0486, Accuracy: 0.9922\n",
      "Validation Loss: 0.2344, Validation Accuracy: 0.9652\n",
      "9.88836470965895e-06 \n",
      "\n",
      "Epoch 92/100 - Loss: 0.0485, Accuracy: 0.9922\n",
      "Validation Loss: 0.2346, Validation Accuracy: 0.9652\n",
      "9.393946474176e-06 \n",
      "\n",
      "Epoch 93/100 - Loss: 0.0485, Accuracy: 0.9922\n",
      "Validation Loss: 0.2347, Validation Accuracy: 0.9654\n",
      "8.924249150467201e-06 \n",
      "\n",
      "Epoch 94/100 - Loss: 0.0484, Accuracy: 0.9923\n",
      "Validation Loss: 0.2347, Validation Accuracy: 0.9654\n",
      "8.478036692943842e-06 \n",
      "\n",
      "Epoch 95/100 - Loss: 0.0484, Accuracy: 0.9923\n",
      "Validation Loss: 0.2348, Validation Accuracy: 0.9654\n",
      "8.054134858296648e-06 \n",
      "\n",
      "Epoch 96/100 - Loss: 0.0483, Accuracy: 0.9922\n",
      "Validation Loss: 0.2349, Validation Accuracy: 0.9654\n",
      "7.651428115381816e-06 \n",
      "\n",
      "Epoch 97/100 - Loss: 0.0483, Accuracy: 0.9922\n",
      "Validation Loss: 0.2350, Validation Accuracy: 0.9654\n",
      "7.2688567096127245e-06 \n",
      "\n",
      "Epoch 98/100 - Loss: 0.0482, Accuracy: 0.9922\n",
      "Validation Loss: 0.2350, Validation Accuracy: 0.9655\n",
      "6.905413874132088e-06 \n",
      "\n",
      "Epoch 99/100 - Loss: 0.0482, Accuracy: 0.9922\n",
      "Validation Loss: 0.2351, Validation Accuracy: 0.9655\n",
      "6.560143180425483e-06 \n",
      "\n",
      "Epoch 100/100 - Loss: 0.0481, Accuracy: 0.9922\n",
      "Validation Loss: 0.2352, Validation Accuracy: 0.9655\n",
      "6.2321360214042085e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize neural network parameters\n",
    "input_size = 784\n",
    "hidden_layer_1_size = 512\n",
    "hidden_layer_2_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = np.random.randn(input_size, hidden_layer_1_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_layer_1_size))\n",
    "\n",
    "w2 = np.random.randn(hidden_layer_1_size, hidden_layer_2_size) * 0.01\n",
    "b2 = np.zeros((1, hidden_layer_2_size))\n",
    "\n",
    "w3 = np.random.randn(hidden_layer_2_size, output_size) * 0.01\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Adam optimizer state variables (momentum, velocity)\n",
    "m_w1, v_w1 = np.zeros_like(w1), np.zeros_like(w1)\n",
    "m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
    "m_w2, v_w2 = np.zeros_like(w2), np.zeros_like(w2)\n",
    "m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
    "m_w3, v_w3 = np.zeros_like(w3), np.zeros_like(w3)\n",
    "m_b3, v_b3 = np.zeros_like(b3), np.zeros_like(b3)\n",
    "\n",
    "beta1, beta2 = 0.9, 0.999  # Adam parameters\n",
    "epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "# Learning rate decay\n",
    "lr_scheduler = lambda epoch: learning_rate * max(0.95 ** epoch, 1e-6)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_array, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # Get the current batch\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = np.dot(X_batch, w1) + b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, w2) + b2\n",
    "        a2 = relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, w3) + b3\n",
    "        y_pred = softmax(z3)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(y_batch, y_pred)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
    "        epoch_accuracy += accuracy\n",
    "\n",
    "        # Backpropagation\n",
    "        # Output layer gradients\n",
    "        dz3 = y_pred - y_batch  # Gradient of loss w.r.t output layer\n",
    "        dw3 = np.dot(a2.T, dz3) / batch_size\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        dz2 = np.dot(dz3, w3.T) * relu_derivative(a2)\n",
    "        dw2 = np.dot(a1.T, dz2) / batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        dz1 = np.dot(dz2, w2.T) * relu_derivative(a1)\n",
    "        dw1 = np.dot(X_batch.T, dz1) / batch_size\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Adam optimizer parameter update\n",
    "        for param, grad, m, v, lr in zip([w1, w2, w3], [dw1, dw2, dw3], [m_w1, m_w2, m_w3], [v_w1, v_w2, v_w3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        for param, grad, m, v, lr in zip([b1, b2, b3], [db1, db2, db3], [m_b1, m_b2, m_b3], [v_b1, v_b2, v_b3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    # Epoch metrics\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss/num_batches:.4f}, Accuracy: {epoch_accuracy/num_batches:.4f}\")\n",
    "    \n",
    "    # Validation loss and accuracy after every epoch\n",
    "    z1_val = np.dot(x_test, w1) + b1\n",
    "    a1_val = relu(z1_val)\n",
    "    z2_val = np.dot(a1_val, w2) + b2\n",
    "    a2_val = relu(z2_val)\n",
    "    z3_val = np.dot(a2_val, w3) + b3\n",
    "    y_pred_val = softmax(z3_val)\n",
    "    \n",
    "    val_loss = cross_entropy_loss(y_test, y_pred_val)\n",
    "    val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_test, axis=1))\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(lr,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30092c-c294-4ea6-937f-594d583bccdd",
   "metadata": {},
   "source": [
    "# Attach image for prediction"
   ]
  },
  {
   "attachments": {
    "3b49974e-f3b6-4045-95ba-fe6e5a53934d.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCAAcABwDASIAAhEBAxEB/8QAGQAAAwADAAAAAAAAAAAAAAAAAwYHBQgJ/8QALRAAAQMCBQIEBgMAAAAAAAAAAgEDBAURAAYHEiEVIhMxQYEIFiNRYXEkYpH/xAAXAQEAAwAAAAAAAAAAAAAAAAABAAID/8QAHBEAAgIDAQEAAAAAAAAAAAAAAAECEQMhMRIT/9oADAMBAAIRAxEAPwDmYw07NlswI4Kbj7otAKJdVVVt5euH/W/TWHpJqTUMgRKmc4qXHirIcMdpA+4wDjjdv6kaj7Yx2l9ZjfNWWKN0SI5K67HfWaW7eo7k7LXtb2xX9VtI8x5x1KzhqNn3ONCydFq9cmuwxq7v15LaPEIqIcdtkSy/bBLJ4ypPg1o1xcWy3LiyYGQ89wbvzh+z/pVVckQafW+oxKxRKohpDqULlkzFe4FX0JLotvzhENtUXtNf9xvCaybiASkVWVQK1DrkJP5EGQEhr7bhW6YtupeTa/8AEHmGRq3kitR8wSKygvVKnSZwNzKfJUU8RtAcJCJtCvtUbpa2IU4tx59VtgsZ2RGLxIsp9gjQrq04oeS2Ty/WKThbtdG9UWHMQVLTLRA9J8zzYz1VrNbGsBT25APdNBtvZuVQVUEnL8p52FL4kaoPH6TAkddeLxX3TdcNLkZkpEvuuCLxiYo/NAf/2Q=="
    },
    "83cbe530-252a-4f1f-8cdc-a864e1ce2bf7.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAA4ADwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6ZApXISJnbgKMk0q8Cs3xNeCx0W4lY4+XNKK1Ao6Z4p0/UtWl061kLXES7mG09M4rcPXmvD/gWraj4t1jUmyVKsgP0evbzndRNWAWnUwGpByahACinZNNPHSl3UwMnxKL9bNRpe3zMjO4E9653x1Het4Rdbpl80oc7ciu8XtiuU+Jb7dAc9gpzVw3Ezl/gRpX2LQbmYjazzSAk/UV6FPd2URxNdwq3oXxXieg+Kb+60T+ytBjIlMzbpASpGeK2LH4ea1e4uL7VLne3JXeDitJpPcEepxXtpJxHcxMfZqs7toGOc15LrPhXV9DtzcWt3NJs5ILV1Xw68Tp4i00K3FzFneD9cVLgraDOzBAp2RUWPWnVkBKpwKx/FmmtqukTQKOSpFa6tTsj0ppgeSfCW3sdE1W7025VUvMtIM9wW4r1xmYHjkVxXjHwZHqswvrBzBfpyGBIHHTIFc9DrHjDSv9HljW4ROAyw/41o/eA9SvVjks5BPjbtOc15B8L7E23jPV3s/+PN0UKR0zu5q/Nf8AirXR9nZBBE3UmLH8q7PwroUehaYIVw0pyWbuc80/hQGy/BozSMM0YrFoCSnDgUUUAG7byf4uKa6RZ+aNT9RRRTuAbI1+4ij6CmnrRRRcBD1ooooA/9k="
    },
    "c4e73d2b-3d0a-424d-bf9b-b4c7bc2120a3.jpg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a6ab1637-90c3-4e2a-9816-ad0c91e6f14f",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "1. Take a photo of the number.\n",
    "    \n",
    "2. Crop the photo such that there is not a lot of empty space between the number and the borders of the image.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840.jpg](attachment:83cbe530-252a-4f1f-8cdc-a864e1ce2bf7.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "4. Resize the image to 28x28 and convert it to a .jpg file.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840 (2).jpg](attachment:c4e73d2b-3d0a-424d-bf9b-b4c7bc2120a3.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "6. Invert the image's colours.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![image.jpg](attachment:3b49974e-f3b6-4045-95ba-fe6e5a53934d.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "8. Copy the path of the image file into the cell and hit run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ff4eb963-fab5-413f-9c28-0d7a9e4889b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 5\n"
     ]
    }
   ],
   "source": [
    "def predict(x_in):\n",
    "    # Forward pass through the network\n",
    "    z1 = np.dot(x_in, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    y_pred = softmax(z3)\n",
    "\n",
    "    # Get the index of the maximum value (predicted class)\n",
    "    return np.argmax(y_pred)\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\vijay\\Documents\\test_images\\img_26399.jpg\"\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Extract features (e.g., flatten the image)\n",
    "x_in = extract_features(image)\n",
    "\n",
    "# Predict the class of the input image\n",
    "predicted_class = predict(x_in)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488029b8-75d7-4f9d-810e-c5cd83543f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
