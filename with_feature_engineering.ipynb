{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7697424f-b391-4b01-aca0-0da917adfb65",
   "metadata": {},
   "source": [
    "# Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766e2b5b-017b-4935-aed0-d94f9fb5c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de982936-c988-4ff1-b120-d650cc72acea",
   "metadata": {},
   "source": [
    "# To read images/ prepare feature vector and convert labels to one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a07f925-eef7-439c-a3a1-f1786b749fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Array Shape: (42000, 245)\n",
      "One-Hot Labels Shape: (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_labels(labels):\n",
    "    \"\"\"\n",
    "    Convert labels into one-hot encoded vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        labels: List or 1D array of integers (0-9).\n",
    "\n",
    "    Returns:\n",
    "        one_hot_labels: 2D NumPy array of shape (n_samples, 10).\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((n_samples, 10))\n",
    "    one_hot_labels[np.arange(n_samples), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Extract 27 covariance features and 18 augmented features (mean magnitude and direction) \n",
    "    from a 28x28 image using a 3x3 grid.\n",
    "\n",
    "    Parameters:\n",
    "        image: 2D NumPy array (28x28) - grayscale image.\n",
    "        weights: 2D NumPy array (3x3) - weights for each grid.\n",
    "\n",
    "    Returns:\n",
    "        features: 1D NumPy array (27 + 18) - covariance features and augmented features for the image.\n",
    "    \"\"\"\n",
    "    grid_size = 28 // 7  # Each grid is 7x7 pixels\n",
    "    features = []\n",
    "    weights = np.array([\n",
    "        [0.23570226, 0.2773501,  0.31622777, 0.33333333, 0.31622777, 0.2773501,  0.23570226],\n",
    "        [0.2773501,  0.35355339, 0.4472136,  0.5,        0.4472136,  0.35355339, 0.2773501],\n",
    "        [0.31622777, 0.4472136,  0.70710678, 1.,         0.70710678, 0.4472136,  0.31622777],\n",
    "        [0.33333333, 0.5,        1.,         1.,         1.,         0.5,        0.33333333],\n",
    "        [0.31622777, 0.4472136,  0.70710678, 1.,         0.70710678, 0.4472136,  0.31622777],\n",
    "        [0.2773501,  0.35355339, 0.4472136,  0.5,        0.4472136,  0.35355339, 0.2773501],\n",
    "        [0.23570226, 0.2773501,  0.31622777, 0.33333333, 0.31622777, 0.2773501,  0.23570226]\n",
    "    ]) #Grid weights, weighted by the inverse of the distance to the centre.\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            # Extract the grid\n",
    "            grid = image[i * grid_size:(i + 1) * grid_size, j * grid_size:(j + 1) * grid_size]\n",
    "\n",
    "            # Find white pixel coordinates\n",
    "            white_pixels = np.argwhere(grid > 0)  # Coordinates of white pixels (row, col)\n",
    "            if len(white_pixels) > 0:\n",
    "                # Calculate deviations from the mean (centroid)\n",
    "                centroid = np.mean(white_pixels, axis=0)  # (row, col)\n",
    "                deviations = white_pixels - centroid  # Deviations (N, 2)\n",
    "                \n",
    "                # Compute covariance terms\n",
    "                sigma_xx = np.mean(deviations[:, 0] ** 2)  # Variance in x\n",
    "                sigma_yy = np.mean(deviations[:, 1] ** 2)  # Variance in y\n",
    "                sigma_xy = np.mean(deviations[:, 0] * deviations[:, 1])  # Covariance\n",
    "\n",
    "                # Compute gradients\n",
    "                gx, gy = np.gradient(grid)  # Gradient along x and y\n",
    "                mag = np.sqrt(gx**2 + gy**2)  # Magnitude of gradient\n",
    "                direction = np.arctan2(gy, gx)  # Direction of gradient\n",
    "\n",
    "                # Mean magnitude and mean direction (ignoring zero-gradient pixels)\n",
    "                mean_mag = np.mean(mag[grid > 0]) if np.any(grid > 0) else 0\n",
    "                mean_dir = np.mean(direction[grid > 0]) if np.any(grid > 0) else 0\n",
    "            else:\n",
    "                # Default if no white pixels\n",
    "                sigma_xx, sigma_yy, sigma_xy = 0, 0, 0\n",
    "                mean_mag, mean_dir = 0, 0\n",
    "\n",
    "            # Apply weight and add features\n",
    "            weight = weights[i, j]\n",
    "            features.extend([\n",
    "                weight * sigma_xx,\n",
    "                weight * sigma_yy,\n",
    "                weight * sigma_xy,\n",
    "                weight * mean_mag,\n",
    "                weight * mean_dir\n",
    "            ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(image_folder_path):\n",
    "    \"\"\"\n",
    "    Process images and labels for a digit recognition model.\n",
    "\n",
    "    Parameters:\n",
    "        image_folder_path: Path to the dataset folder containing subfolders 0-9.\n",
    "\n",
    "    Returns:\n",
    "        features_array: 2D NumPy array (n_samples, 18) - extracted features.\n",
    "        one_hot_labels: 2D NumPy array (n_samples, 10) - one-hot encoded labels.\n",
    "    \"\"\"\n",
    "        \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Traverse folders (0-9)\n",
    "    for label in range(10):\n",
    "        label_folder = os.path.join(image_folder_path, str(label))\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Open and process the image\n",
    "            image = Image.open(file_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "            image = image.point(lambda p: 255 if p >= 128 else 0, '1')\n",
    "            image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "            features = extract_features(image)\n",
    "\n",
    "            # Append features and label\n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    features_array = np.array(features_list)\n",
    "    one_hot_labels = one_hot_encode_labels(labels_list)\n",
    "\n",
    "    return features_array, one_hot_labels\n",
    "\n",
    "# Example usage\n",
    "image_folder_path = r\"\"  # Replace with your dataset path\n",
    "features_array, one_hot_labels = preprocess(image_folder_path)\n",
    "\n",
    "print(\"Feature Array Shape:\", features_array.shape)  # Should be (n_samples, 245)\n",
    "print(\"One-Hot Labels Shape:\", one_hot_labels.shape)  # Should be (n_samples, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4bb63-5503-446b-9adb-eca48643db14",
   "metadata": {},
   "source": [
    "# ReLu and it's derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c979280-568f-46a7-8324-d51cc3c3d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0ad7e-fd35-4572-9068-d93ecd9ebc9b",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd3f9d76-5100-49aa-adab-d12a45f968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cc4f9-e4c5-4a84-9d18-f2672503c0a7",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f2cf8c-eb63-4d1f-b28c-12f2e24484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Categorical cross-entropy loss with small value epsilon to avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8c378-84ca-4064-aa05-16397a02138d",
   "metadata": {},
   "source": [
    "# Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd7c1744-de58-4583-8fe9-e5d75d7deed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Loss: 0.7325, Accuracy: 0.7900\n",
      "Validation Loss: 0.3590, Validation Accuracy: 0.8892\n",
      "Learning rate: 0.001 \n",
      "\n",
      "Epoch 2/50 - Loss: 0.3083, Accuracy: 0.9054\n",
      "Validation Loss: 0.2811, Validation Accuracy: 0.9129\n",
      "Learning rate: 0.00095 \n",
      "\n",
      "Epoch 3/50 - Loss: 0.2585, Accuracy: 0.9196\n",
      "Validation Loss: 0.2512, Validation Accuracy: 0.9227\n",
      "Learning rate: 0.0009025 \n",
      "\n",
      "Epoch 4/50 - Loss: 0.2309, Accuracy: 0.9291\n",
      "Validation Loss: 0.2316, Validation Accuracy: 0.9288\n",
      "Learning rate: 0.000857375 \n",
      "\n",
      "Epoch 5/50 - Loss: 0.2106, Accuracy: 0.9358\n",
      "Validation Loss: 0.2187, Validation Accuracy: 0.9332\n",
      "Learning rate: 0.0008145062499999999 \n",
      "\n",
      "Epoch 6/50 - Loss: 0.1949, Accuracy: 0.9408\n",
      "Validation Loss: 0.2090, Validation Accuracy: 0.9364\n",
      "Learning rate: 0.0007737809374999998 \n",
      "\n",
      "Epoch 7/50 - Loss: 0.1826, Accuracy: 0.9450\n",
      "Validation Loss: 0.2009, Validation Accuracy: 0.9392\n",
      "Learning rate: 0.0007350918906249999 \n",
      "\n",
      "Epoch 8/50 - Loss: 0.1728, Accuracy: 0.9479\n",
      "Validation Loss: 0.1953, Validation Accuracy: 0.9414\n",
      "Learning rate: 0.0006983372960937497 \n",
      "\n",
      "Epoch 9/50 - Loss: 0.1646, Accuracy: 0.9509\n",
      "Validation Loss: 0.1902, Validation Accuracy: 0.9427\n",
      "Learning rate: 0.0006634204312890623 \n",
      "\n",
      "Epoch 10/50 - Loss: 0.1577, Accuracy: 0.9532\n",
      "Validation Loss: 0.1863, Validation Accuracy: 0.9438\n",
      "Learning rate: 0.0006302494097246091 \n",
      "\n",
      "Epoch 11/50 - Loss: 0.1518, Accuracy: 0.9555\n",
      "Validation Loss: 0.1835, Validation Accuracy: 0.9455\n",
      "Learning rate: 0.0005987369392383787 \n",
      "\n",
      "Epoch 12/50 - Loss: 0.1467, Accuracy: 0.9570\n",
      "Validation Loss: 0.1809, Validation Accuracy: 0.9476\n",
      "Learning rate: 0.0005688000922764596 \n",
      "\n",
      "Epoch 13/50 - Loss: 0.1423, Accuracy: 0.9580\n",
      "Validation Loss: 0.1792, Validation Accuracy: 0.9486\n",
      "Learning rate: 0.0005403600876626366 \n",
      "\n",
      "Epoch 14/50 - Loss: 0.1384, Accuracy: 0.9603\n",
      "Validation Loss: 0.1777, Validation Accuracy: 0.9494\n",
      "Learning rate: 0.0005133420832795048 \n",
      "\n",
      "Epoch 15/50 - Loss: 0.1351, Accuracy: 0.9616\n",
      "Validation Loss: 0.1766, Validation Accuracy: 0.9511\n",
      "Learning rate: 0.00048767497911552955 \n",
      "\n",
      "Epoch 16/50 - Loss: 0.1322, Accuracy: 0.9626\n",
      "Validation Loss: 0.1756, Validation Accuracy: 0.9507\n",
      "Learning rate: 0.000463291230159753 \n",
      "\n",
      "Epoch 17/50 - Loss: 0.1296, Accuracy: 0.9637\n",
      "Validation Loss: 0.1751, Validation Accuracy: 0.9511\n",
      "Learning rate: 0.00044012666865176535 \n",
      "\n",
      "Epoch 18/50 - Loss: 0.1272, Accuracy: 0.9644\n",
      "Validation Loss: 0.1747, Validation Accuracy: 0.9514\n",
      "Learning rate: 0.0004181203352191771 \n",
      "\n",
      "Epoch 19/50 - Loss: 0.1253, Accuracy: 0.9655\n",
      "Validation Loss: 0.1748, Validation Accuracy: 0.9519\n",
      "Learning rate: 0.0003972143184582182 \n",
      "\n",
      "Epoch 20/50 - Loss: 0.1233, Accuracy: 0.9660\n",
      "Validation Loss: 0.1751, Validation Accuracy: 0.9523\n",
      "Learning rate: 0.00037735360253530727 \n",
      "\n",
      "Epoch 21/50 - Loss: 0.1217, Accuracy: 0.9669\n",
      "Validation Loss: 0.1753, Validation Accuracy: 0.9531\n",
      "Learning rate: 0.0003584859224085419 \n",
      "\n",
      "Epoch 22/50 - Loss: 0.1202, Accuracy: 0.9678\n",
      "Validation Loss: 0.1760, Validation Accuracy: 0.9531\n",
      "Learning rate: 0.0003405616262881148 \n",
      "\n",
      "Epoch 23/50 - Loss: 0.1189, Accuracy: 0.9684\n",
      "Validation Loss: 0.1765, Validation Accuracy: 0.9533\n",
      "Learning rate: 0.000323533544973709 \n",
      "\n",
      "Epoch 24/50 - Loss: 0.1177, Accuracy: 0.9690\n",
      "Validation Loss: 0.1769, Validation Accuracy: 0.9531\n",
      "Learning rate: 0.00030735686772502356 \n",
      "\n",
      "Epoch 25/50 - Loss: 0.1164, Accuracy: 0.9698\n",
      "Validation Loss: 0.1773, Validation Accuracy: 0.9535\n",
      "Learning rate: 0.0002919890243387724 \n",
      "\n",
      "Epoch 26/50 - Loss: 0.1151, Accuracy: 0.9707\n",
      "Validation Loss: 0.1778, Validation Accuracy: 0.9535\n",
      "Learning rate: 0.00027738957312183375 \n",
      "\n",
      "Epoch 27/50 - Loss: 0.1141, Accuracy: 0.9713\n",
      "Validation Loss: 0.1780, Validation Accuracy: 0.9535\n",
      "Learning rate: 0.00026352009446574203 \n",
      "\n",
      "Epoch 28/50 - Loss: 0.1131, Accuracy: 0.9718\n",
      "Validation Loss: 0.1784, Validation Accuracy: 0.9535\n",
      "Learning rate: 0.00025034408974245495 \n",
      "\n",
      "Epoch 29/50 - Loss: 0.1121, Accuracy: 0.9721\n",
      "Validation Loss: 0.1788, Validation Accuracy: 0.9540\n",
      "Learning rate: 0.00023782688525533216 \n",
      "\n",
      "Epoch 30/50 - Loss: 0.1113, Accuracy: 0.9724\n",
      "Validation Loss: 0.1792, Validation Accuracy: 0.9543\n",
      "Learning rate: 0.00022593554099256555 \n",
      "\n",
      "Epoch 31/50 - Loss: 0.1105, Accuracy: 0.9727\n",
      "Validation Loss: 0.1797, Validation Accuracy: 0.9551\n",
      "Learning rate: 0.00021463876394293727 \n",
      "\n",
      "Epoch 32/50 - Loss: 0.1098, Accuracy: 0.9733\n",
      "Validation Loss: 0.1804, Validation Accuracy: 0.9557\n",
      "Learning rate: 0.00020390682574579038 \n",
      "\n",
      "Epoch 33/50 - Loss: 0.1092, Accuracy: 0.9736\n",
      "Validation Loss: 0.1811, Validation Accuracy: 0.9560\n",
      "Learning rate: 0.00019371148445850088 \n",
      "\n",
      "Epoch 34/50 - Loss: 0.1086, Accuracy: 0.9739\n",
      "Validation Loss: 0.1817, Validation Accuracy: 0.9557\n",
      "Learning rate: 0.00018402591023557584 \n",
      "\n",
      "Epoch 35/50 - Loss: 0.1080, Accuracy: 0.9743\n",
      "Validation Loss: 0.1823, Validation Accuracy: 0.9557\n",
      "Learning rate: 0.000174824614723797 \n",
      "\n",
      "Epoch 36/50 - Loss: 0.1074, Accuracy: 0.9746\n",
      "Validation Loss: 0.1827, Validation Accuracy: 0.9557\n",
      "Learning rate: 0.00016608338398760718 \n",
      "\n",
      "Epoch 37/50 - Loss: 0.1069, Accuracy: 0.9749\n",
      "Validation Loss: 0.1833, Validation Accuracy: 0.9560\n",
      "Learning rate: 0.0001577792147882268 \n",
      "\n",
      "Epoch 38/50 - Loss: 0.1063, Accuracy: 0.9752\n",
      "Validation Loss: 0.1835, Validation Accuracy: 0.9558\n",
      "Learning rate: 0.00014989025404881545 \n",
      "\n",
      "Epoch 39/50 - Loss: 0.1057, Accuracy: 0.9753\n",
      "Validation Loss: 0.1838, Validation Accuracy: 0.9555\n",
      "Learning rate: 0.00014239574134637466 \n",
      "\n",
      "Epoch 40/50 - Loss: 0.1052, Accuracy: 0.9755\n",
      "Validation Loss: 0.1844, Validation Accuracy: 0.9556\n",
      "Learning rate: 0.00013527595427905592 \n",
      "\n",
      "Epoch 41/50 - Loss: 0.1047, Accuracy: 0.9757\n",
      "Validation Loss: 0.1848, Validation Accuracy: 0.9556\n",
      "Learning rate: 0.00012851215656510312 \n",
      "\n",
      "Epoch 42/50 - Loss: 0.1041, Accuracy: 0.9759\n",
      "Validation Loss: 0.1852, Validation Accuracy: 0.9558\n",
      "Learning rate: 0.00012208654873684796 \n",
      "\n",
      "Epoch 43/50 - Loss: 0.1037, Accuracy: 0.9760\n",
      "Validation Loss: 0.1857, Validation Accuracy: 0.9561\n",
      "Learning rate: 0.00011598222130000556 \n",
      "\n",
      "Epoch 44/50 - Loss: 0.1032, Accuracy: 0.9762\n",
      "Validation Loss: 0.1862, Validation Accuracy: 0.9562\n",
      "Learning rate: 0.00011018311023500529 \n",
      "\n",
      "Epoch 45/50 - Loss: 0.1028, Accuracy: 0.9764\n",
      "Validation Loss: 0.1867, Validation Accuracy: 0.9564\n",
      "Learning rate: 0.00010467395472325501 \n",
      "\n",
      "Epoch 46/50 - Loss: 0.1023, Accuracy: 0.9766\n",
      "Validation Loss: 0.1871, Validation Accuracy: 0.9562\n",
      "Learning rate: 9.944025698709225e-05 \n",
      "\n",
      "Epoch 47/50 - Loss: 0.1019, Accuracy: 0.9767\n",
      "Validation Loss: 0.1877, Validation Accuracy: 0.9561\n",
      "Learning rate: 9.446824413773763e-05 \n",
      "\n",
      "Epoch 48/50 - Loss: 0.1015, Accuracy: 0.9767\n",
      "Validation Loss: 0.1882, Validation Accuracy: 0.9558\n",
      "Learning rate: 8.974483193085076e-05 \n",
      "\n",
      "Epoch 49/50 - Loss: 0.1011, Accuracy: 0.9767\n",
      "Validation Loss: 0.1886, Validation Accuracy: 0.9558\n",
      "Learning rate: 8.52575903343082e-05 \n",
      "\n",
      "Epoch 50/50 - Loss: 0.1008, Accuracy: 0.9767\n",
      "Validation Loss: 0.1892, Validation Accuracy: 0.9560\n",
      "Learning rate: 8.099471081759279e-05 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize neural network parameters\n",
    "input_size = 245\n",
    "hidden_layer_1_size = 512\n",
    "hidden_layer_2_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = np.random.randn(input_size, hidden_layer_1_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_layer_1_size))\n",
    "\n",
    "w2 = np.random.randn(hidden_layer_1_size, hidden_layer_2_size) * 0.01\n",
    "b2 = np.zeros((1, hidden_layer_2_size))\n",
    "\n",
    "w3 = np.random.randn(hidden_layer_2_size, output_size) * 0.01\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Adam optimizer state variables (momentum, velocity)\n",
    "m_w1, v_w1 = np.zeros_like(w1), np.zeros_like(w1)\n",
    "m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
    "m_w2, v_w2 = np.zeros_like(w2), np.zeros_like(w2)\n",
    "m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
    "m_w3, v_w3 = np.zeros_like(w3), np.zeros_like(w3)\n",
    "m_b3, v_b3 = np.zeros_like(b3), np.zeros_like(b3)\n",
    "\n",
    "beta1, beta2 = 0.9, 0.999  # Adam parameters\n",
    "epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "# Learning rate decay\n",
    "lr_scheduler = lambda epoch: learning_rate * max(0.95 ** epoch, 1e-6)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_array, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # Get the current batch\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = np.dot(X_batch, w1) + b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, w2) + b2\n",
    "        a2 = relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, w3) + b3\n",
    "        y_pred = softmax(z3)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(y_batch, y_pred)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
    "        epoch_accuracy += accuracy\n",
    "\n",
    "        # Backpropagation\n",
    "        # Output layer gradients\n",
    "        dz3 = y_pred - y_batch  # Gradient of loss w.r.t output layer\n",
    "        dw3 = np.dot(a2.T, dz3) / batch_size\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        dz2 = np.dot(dz3, w3.T) * relu_derivative(a2)\n",
    "        dw2 = np.dot(a1.T, dz2) / batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        dz1 = np.dot(dz2, w2.T) * relu_derivative(a1)\n",
    "        dw1 = np.dot(X_batch.T, dz1) / batch_size\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Adam optimizer parameter update\n",
    "        for param, grad, m, v, lr in zip([w1, w2, w3], [dw1, dw2, dw3], [m_w1, m_w2, m_w3], [v_w1, v_w2, v_w3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        for param, grad, m, v, lr in zip([b1, b2, b3], [db1, db2, db3], [m_b1, m_b2, m_b3], [v_b1, v_b2, v_b3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    # Epoch metrics\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss/num_batches:.4f}, Accuracy: {epoch_accuracy/num_batches:.4f}\")\n",
    "    \n",
    "    # Validation loss and accuracy after every epoch\n",
    "    z1_val = np.dot(x_test, w1) + b1\n",
    "    a1_val = relu(z1_val)\n",
    "    z2_val = np.dot(a1_val, w2) + b2\n",
    "    a2_val = relu(z2_val)\n",
    "    z3_val = np.dot(a2_val, w3) + b3\n",
    "    y_pred_val = softmax(z3_val)\n",
    "    \n",
    "    val_loss = cross_entropy_loss(y_test, y_pred_val)\n",
    "    val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_test, axis=1))\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Learning rate: {lr}\",\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff2c28-b06a-4396-b5ff-7fa2734a8a1b",
   "metadata": {},
   "source": [
    "# TensorFlow model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a182f98-b13e-4e08-a129-5163ac8cf6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7420 - loss: 0.9741 - val_accuracy: 0.9256 - val_loss: 0.2448\n",
      "Epoch 2/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9355 - loss: 0.2119 - val_accuracy: 0.9398 - val_loss: 0.1951\n",
      "Epoch 3/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9497 - loss: 0.1672 - val_accuracy: 0.9495 - val_loss: 0.1636\n",
      "Epoch 4/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9593 - loss: 0.1304 - val_accuracy: 0.9467 - val_loss: 0.1651\n",
      "Epoch 5/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1130 - val_accuracy: 0.9557 - val_loss: 0.1441\n",
      "Epoch 6/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9712 - loss: 0.0970 - val_accuracy: 0.9579 - val_loss: 0.1346\n",
      "Epoch 7/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0855 - val_accuracy: 0.9599 - val_loss: 0.1329\n",
      "Epoch 8/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9782 - loss: 0.0762 - val_accuracy: 0.9619 - val_loss: 0.1221\n",
      "Epoch 9/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9808 - loss: 0.0650 - val_accuracy: 0.9607 - val_loss: 0.1224\n",
      "Epoch 10/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0582 - val_accuracy: 0.9638 - val_loss: 0.1202\n",
      "Epoch 11/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9850 - loss: 0.0528 - val_accuracy: 0.9613 - val_loss: 0.1220\n",
      "Epoch 12/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9888 - loss: 0.0415 - val_accuracy: 0.9652 - val_loss: 0.1142\n",
      "Epoch 13/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9897 - loss: 0.0356 - val_accuracy: 0.9632 - val_loss: 0.1204\n",
      "Epoch 14/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9895 - loss: 0.0366 - val_accuracy: 0.9652 - val_loss: 0.1173\n",
      "Epoch 15/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9921 - loss: 0.0298 - val_accuracy: 0.9626 - val_loss: 0.1274\n",
      "Epoch 16/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9932 - loss: 0.0252 - val_accuracy: 0.9645 - val_loss: 0.1266\n",
      "Epoch 17/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9944 - loss: 0.0228 - val_accuracy: 0.9649 - val_loss: 0.1266\n",
      "Epoch 18/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9960 - loss: 0.0166 - val_accuracy: 0.9664 - val_loss: 0.1229\n",
      "Epoch 19/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0142 - val_accuracy: 0.9656 - val_loss: 0.1255\n",
      "Epoch 20/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9972 - loss: 0.0131 - val_accuracy: 0.9671 - val_loss: 0.1300\n",
      "Epoch 21/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0120 - val_accuracy: 0.9663 - val_loss: 0.1323\n",
      "Epoch 22/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9982 - loss: 0.0095 - val_accuracy: 0.9646 - val_loss: 0.1371\n",
      "Epoch 23/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9978 - loss: 0.0086 - val_accuracy: 0.9624 - val_loss: 0.1519\n",
      "Epoch 24/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9977 - loss: 0.0105 - val_accuracy: 0.9675 - val_loss: 0.1353\n",
      "Epoch 25/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9993 - loss: 0.0057 - val_accuracy: 0.9668 - val_loss: 0.1465\n",
      "Epoch 26/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0067 - val_accuracy: 0.9651 - val_loss: 0.1519\n",
      "Epoch 27/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9985 - loss: 0.0063 - val_accuracy: 0.9657 - val_loss: 0.1509\n",
      "Epoch 28/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9987 - loss: 0.0054 - val_accuracy: 0.9677 - val_loss: 0.1457\n",
      "Epoch 29/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0032 - val_accuracy: 0.9674 - val_loss: 0.1543\n",
      "Epoch 30/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9992 - loss: 0.0039 - val_accuracy: 0.9669 - val_loss: 0.1582\n",
      "Epoch 31/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9969 - loss: 0.0107 - val_accuracy: 0.9624 - val_loss: 0.1808\n",
      "Epoch 32/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9958 - loss: 0.0112 - val_accuracy: 0.9674 - val_loss: 0.1646\n",
      "Epoch 33/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9996 - loss: 0.0032 - val_accuracy: 0.9669 - val_loss: 0.1575\n",
      "Epoch 34/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.9679 - val_loss: 0.1579\n",
      "Epoch 35/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 6.8413e-04 - val_accuracy: 0.9673 - val_loss: 0.1622\n",
      "Epoch 36/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 5.1154e-04 - val_accuracy: 0.9688 - val_loss: 0.1610\n",
      "Epoch 37/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.4633e-04 - val_accuracy: 0.9682 - val_loss: 0.1632\n",
      "Epoch 38/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 4.1016e-04 - val_accuracy: 0.9681 - val_loss: 0.1636\n",
      "Epoch 39/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.8093e-04 - val_accuracy: 0.9685 - val_loss: 0.1667\n",
      "Epoch 40/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 3.0890e-04 - val_accuracy: 0.9685 - val_loss: 0.1682\n",
      "Epoch 41/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.8257e-04 - val_accuracy: 0.9688 - val_loss: 0.1699\n",
      "Epoch 42/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.5321e-04 - val_accuracy: 0.9680 - val_loss: 0.1716\n",
      "Epoch 43/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.4437e-04 - val_accuracy: 0.9685 - val_loss: 0.1763\n",
      "Epoch 44/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.8705e-04 - val_accuracy: 0.9683 - val_loss: 0.1745\n",
      "Epoch 45/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.7943e-04 - val_accuracy: 0.9690 - val_loss: 0.1762\n",
      "Epoch 46/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.6542e-04 - val_accuracy: 0.9683 - val_loss: 0.1778\n",
      "Epoch 47/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.4767e-04 - val_accuracy: 0.9682 - val_loss: 0.1823\n",
      "Epoch 48/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9926 - loss: 0.0266 - val_accuracy: 0.9637 - val_loss: 0.1697\n",
      "Epoch 49/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9966 - loss: 0.0108 - val_accuracy: 0.9679 - val_loss: 0.1626\n",
      "Epoch 50/50\n",
      "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9995 - loss: 0.0026 - val_accuracy: 0.9685 - val_loss: 0.1651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x200bc19b5c0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(245,)),               # input layer\n",
    "    Dense(256, activation='relu'),     # hidden layer 1\n",
    "    Dense(64, activation='relu'),      # hidden layer 2\n",
    "    Dense(10, activation='softmax')    # output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=128, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63349c-b803-499d-ad3b-12739764ceb8",
   "metadata": {},
   "source": [
    "# Attach image to predict"
   ]
  },
  {
   "attachments": {
    "6878b302-6f1b-4cce-bc73-a88eb52e5985.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAA4ADwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6ZApXISJnbgKMk0q8Cs3xNeCx0W4lY4+XNKK1Ao6Z4p0/UtWl061kLXES7mG09M4rcPXmvD/gWraj4t1jUmyVKsgP0evbzndRNWAWnUwGpByahACinZNNPHSl3UwMnxKL9bNRpe3zMjO4E9653x1Het4Rdbpl80oc7ciu8XtiuU+Jb7dAc9gpzVw3Ezl/gRpX2LQbmYjazzSAk/UV6FPd2URxNdwq3oXxXieg+Kb+60T+ytBjIlMzbpASpGeK2LH4ea1e4uL7VLne3JXeDitJpPcEepxXtpJxHcxMfZqs7toGOc15LrPhXV9DtzcWt3NJs5ILV1Xw68Tp4i00K3FzFneD9cVLgraDOzBAp2RUWPWnVkBKpwKx/FmmtqukTQKOSpFa6tTsj0ppgeSfCW3sdE1W7025VUvMtIM9wW4r1xmYHjkVxXjHwZHqswvrBzBfpyGBIHHTIFc9DrHjDSv9HljW4ROAyw/41o/eA9SvVjks5BPjbtOc15B8L7E23jPV3s/+PN0UKR0zu5q/Nf8AirXR9nZBBE3UmLH8q7PwroUehaYIVw0pyWbuc80/hQGy/BozSMM0YrFoCSnDgUUUAG7byf4uKa6RZ+aNT9RRRTuAbI1+4ij6CmnrRRRcBD1ooooA/9k="
    },
    "b24061ed-10ac-4eed-a1d9-f3b2cb224ae3.jpg": {
     "image/jpeg": "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"
    },
    "e230ce8d-ebf8-466b-893c-b785a5e1e7fe.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCAAcABwDASIAAhEBAxEB/8QAGQAAAwADAAAAAAAAAAAAAAAAAwYHBQgJ/8QALRAAAQMCBQIEBgMAAAAAAAAAAgEDBAURAAYHEiEVIhMxQYEIFiNRYXEkYpH/xAAXAQEAAwAAAAAAAAAAAAAAAAABAAID/8QAHBEAAgIDAQEAAAAAAAAAAAAAAAECEQMhMRIT/9oADAMBAAIRAxEAPwDmYw07NlswI4Kbj7otAKJdVVVt5euH/W/TWHpJqTUMgRKmc4qXHirIcMdpA+4wDjjdv6kaj7Yx2l9ZjfNWWKN0SI5K67HfWaW7eo7k7LXtb2xX9VtI8x5x1KzhqNn3ONCydFq9cmuwxq7v15LaPEIqIcdtkSy/bBLJ4ypPg1o1xcWy3LiyYGQ89wbvzh+z/pVVckQafW+oxKxRKohpDqULlkzFe4FX0JLotvzhENtUXtNf9xvCaybiASkVWVQK1DrkJP5EGQEhr7bhW6YtupeTa/8AEHmGRq3kitR8wSKygvVKnSZwNzKfJUU8RtAcJCJtCvtUbpa2IU4tx59VtgsZ2RGLxIsp9gjQrq04oeS2Ty/WKThbtdG9UWHMQVLTLRA9J8zzYz1VrNbGsBT25APdNBtvZuVQVUEnL8p52FL4kaoPH6TAkddeLxX3TdcNLkZkpEvuuCLxiYo/NAf/2Q=="
    }
   },
   "cell_type": "markdown",
   "id": "79b78481-6567-46f3-9037-2fa443387268",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "1. Take a photo of the number.\n",
    "    \n",
    "2. Crop the photo such that there is not a lot of empty space between the number and the borders of the image.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840.jpg](attachment:6878b302-6f1b-4cce-bc73-a88eb52e5985.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "4. Resize the image to 28x28 and convert it to a .jpg file.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840 (2).jpg](attachment:b24061ed-10ac-4eed-a1d9-f3b2cb224ae3.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "6. Invert the image's colours.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![image.jpg](attachment:e230ce8d-ebf8-466b-893c-b785a5e1e7fe.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "8. Copy the path of the image file into the cell and hit run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c112a014-d303-4771-8e80-d2c176e84c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 6\n"
     ]
    }
   ],
   "source": [
    "def predict(x_in):\n",
    "    # Forward pass through the network\n",
    "    z1 = np.dot(x_in, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    y_pred = softmax(z3)\n",
    "\n",
    "    # Get the index of the maximum value (predicted number)\n",
    "    return np.argmax(y_pred)\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"\" # Paste image file path here.\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "image = image.point(lambda p: 255 if p >= 128 else 0, '1')\n",
    "image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Extract features (e.g., flatten the image)\n",
    "x_in = extract_features(image)\n",
    "\n",
    "# Predict the class of the input image\n",
    "predicted_class = predict(x_in)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
