{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7697424f-b391-4b01-aca0-0da917adfb65",
   "metadata": {},
   "source": [
    "# Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "766e2b5b-017b-4935-aed0-d94f9fb5c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de982936-c988-4ff1-b120-d650cc72acea",
   "metadata": {},
   "source": [
    "# To read images/ prepare feature vector and convert labels to one hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a07f925-eef7-439c-a3a1-f1786b749fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Array Shape: (42000, 245)\n",
      "One-Hot Labels Shape: (42000, 10)\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_labels(labels):\n",
    "    \"\"\"\n",
    "    Convert labels into one-hot encoded vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        labels: List or 1D array of integers (0-9).\n",
    "\n",
    "    Returns:\n",
    "        one_hot_labels: 2D NumPy array of shape (n_samples, 10).\n",
    "    \"\"\"\n",
    "    n_samples = len(labels)\n",
    "    one_hot_labels = np.zeros((n_samples, 10))\n",
    "    one_hot_labels[np.arange(n_samples), labels] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def extract_features(image):\n",
    "    \"\"\"\n",
    "    Extract 27 covariance features and 18 augmented features (mean magnitude and direction) \n",
    "    from a 28x28 image using a 3x3 grid.\n",
    "\n",
    "    Parameters:\n",
    "        image: 2D NumPy array (28x28) - grayscale image.\n",
    "        weights: 2D NumPy array (3x3) - weights for each grid.\n",
    "\n",
    "    Returns:\n",
    "        features: 1D NumPy array (27 + 18) - covariance features and augmented features for the image.\n",
    "    \"\"\"\n",
    "    grid_size = 28 // 7  # Each grid is 7x7 pixels\n",
    "    features = []\n",
    "    weights = np.array([\n",
    "        [0.23570226, 0.2773501,  0.31622777, 0.33333333, 0.31622777, 0.2773501,  0.23570226],\n",
    "        [0.2773501,  0.35355339, 0.4472136,  0.5,        0.4472136,  0.35355339, 0.2773501],\n",
    "        [0.31622777, 0.4472136,  0.70710678, 1.,         0.70710678, 0.4472136,  0.31622777],\n",
    "        [0.33333333, 0.5,        1.,         1.,         1.,         0.5,        0.33333333],\n",
    "        [0.31622777, 0.4472136,  0.70710678, 1.,         0.70710678, 0.4472136,  0.31622777],\n",
    "        [0.2773501,  0.35355339, 0.4472136,  0.5,        0.4472136,  0.35355339, 0.2773501],\n",
    "        [0.23570226, 0.2773501,  0.31622777, 0.33333333, 0.31622777, 0.2773501,  0.23570226]\n",
    "    ]) #Grid weights, weighted by the inverse of the distance to the centre.\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            # Extract the grid\n",
    "            grid = image[i * grid_size:(i + 1) * grid_size, j * grid_size:(j + 1) * grid_size]\n",
    "\n",
    "            # Find white pixel coordinates\n",
    "            white_pixels = np.argwhere(grid > 0)  # Coordinates of white pixels (row, col)\n",
    "            if len(white_pixels) > 0:\n",
    "                # Calculate deviations from the mean (centroid)\n",
    "                centroid = np.mean(white_pixels, axis=0)  # (row, col)\n",
    "                deviations = white_pixels - centroid  # Deviations (N, 2)\n",
    "                \n",
    "                # Compute covariance terms\n",
    "                sigma_xx = np.mean(deviations[:, 0] ** 2)  # Variance in x\n",
    "                sigma_yy = np.mean(deviations[:, 1] ** 2)  # Variance in y\n",
    "                sigma_xy = np.mean(deviations[:, 0] * deviations[:, 1])  # Covariance\n",
    "\n",
    "                # Compute gradients\n",
    "                gx, gy = np.gradient(grid)  # Gradient along x and y\n",
    "                mag = np.sqrt(gx**2 + gy**2)  # Magnitude of gradient\n",
    "                direction = np.arctan2(gy, gx)  # Direction of gradient\n",
    "\n",
    "                # Mean magnitude and mean direction (ignoring zero-gradient pixels)\n",
    "                mean_mag = np.mean(mag[grid > 0]) if np.any(grid > 0) else 0\n",
    "                mean_dir = np.mean(direction[grid > 0]) if np.any(grid > 0) else 0\n",
    "            else:\n",
    "                # Default if no white pixels\n",
    "                sigma_xx, sigma_yy, sigma_xy = 0, 0, 0\n",
    "                mean_mag, mean_dir = 0, 0\n",
    "\n",
    "            # Apply weight and add features\n",
    "            weight = weights[i, j]\n",
    "            features.extend([\n",
    "                weight * sigma_xx,\n",
    "                weight * sigma_yy,\n",
    "                weight * sigma_xy,\n",
    "                weight * mean_mag,\n",
    "                weight * mean_dir\n",
    "            ])\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(image_folder_path):\n",
    "    \"\"\"\n",
    "    Process images and labels for a digit recognition model.\n",
    "\n",
    "    Parameters:\n",
    "        image_folder_path: Path to the dataset folder containing subfolders 0-9.\n",
    "\n",
    "    Returns:\n",
    "        features_array: 2D NumPy array (n_samples, 18) - extracted features.\n",
    "        one_hot_labels: 2D NumPy array (n_samples, 10) - one-hot encoded labels.\n",
    "    \"\"\"\n",
    "        \n",
    "    features_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Traverse folders (0-9)\n",
    "    for label in range(10):\n",
    "        label_folder = os.path.join(image_folder_path, str(label))\n",
    "        for filename in os.listdir(label_folder):\n",
    "            file_path = os.path.join(label_folder, filename)\n",
    "\n",
    "            # Open and process the image\n",
    "            image = Image.open(file_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "            image = image.point(lambda p: 255 if p >= 128 else 0, '1')\n",
    "            image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "            features = extract_features(image)\n",
    "\n",
    "            # Append features and label\n",
    "            features_list.append(features)\n",
    "            labels_list.append(label)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    features_array = np.array(features_list)\n",
    "    one_hot_labels = one_hot_encode_labels(labels_list)\n",
    "\n",
    "    return features_array, one_hot_labels\n",
    "\n",
    "# Example usage\n",
    "image_folder_path = r\"C:\\Users\\vijay\\Documents\\digit_rec\\trainingSet\\trainingSet\"  # Replace with your dataset path\n",
    "features_array, one_hot_labels = preprocess(image_folder_path)\n",
    "\n",
    "print(\"Feature Array Shape:\", features_array.shape)  # Should be (n_samples, 245)\n",
    "print(\"One-Hot Labels Shape:\", one_hot_labels.shape)  # Should be (n_samples, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef4bb63-5503-446b-9adb-eca48643db14",
   "metadata": {},
   "source": [
    "# ReLu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c979280-568f-46a7-8324-d51cc3c3d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a0ad7e-fd35-4572-9068-d93ecd9ebc9b",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd3f9d76-5100-49aa-adab-d12a45f968c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cc4f9-e4c5-4a84-9d18-f2672503c0a7",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f2cf8c-eb63-4d1f-b28c-12f2e24484d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Categorical cross-entropy loss with epsilon to avoid log(0)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred + 1e-8), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8c378-84ca-4064-aa05-16397a02138d",
   "metadata": {},
   "source": [
    "# Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd7c1744-de58-4583-8fe9-e5d75d7deed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 0.7325, Accuracy: 0.7900\n",
      "Validation Loss: 0.3590, Validation Accuracy: 0.8892\n",
      "0.001 \n",
      "\n",
      "Epoch 2/100 - Loss: 0.3083, Accuracy: 0.9054\n",
      "Validation Loss: 0.2811, Validation Accuracy: 0.9129\n",
      "0.00095 \n",
      "\n",
      "Epoch 3/100 - Loss: 0.2585, Accuracy: 0.9196\n",
      "Validation Loss: 0.2512, Validation Accuracy: 0.9227\n",
      "0.0009025 \n",
      "\n",
      "Epoch 4/100 - Loss: 0.2309, Accuracy: 0.9291\n",
      "Validation Loss: 0.2316, Validation Accuracy: 0.9288\n",
      "0.000857375 \n",
      "\n",
      "Epoch 5/100 - Loss: 0.2106, Accuracy: 0.9358\n",
      "Validation Loss: 0.2187, Validation Accuracy: 0.9332\n",
      "0.0008145062499999999 \n",
      "\n",
      "Epoch 6/100 - Loss: 0.1949, Accuracy: 0.9408\n",
      "Validation Loss: 0.2090, Validation Accuracy: 0.9364\n",
      "0.0007737809374999998 \n",
      "\n",
      "Epoch 7/100 - Loss: 0.1826, Accuracy: 0.9450\n",
      "Validation Loss: 0.2009, Validation Accuracy: 0.9392\n",
      "0.0007350918906249999 \n",
      "\n",
      "Epoch 8/100 - Loss: 0.1728, Accuracy: 0.9479\n",
      "Validation Loss: 0.1953, Validation Accuracy: 0.9414\n",
      "0.0006983372960937497 \n",
      "\n",
      "Epoch 9/100 - Loss: 0.1646, Accuracy: 0.9509\n",
      "Validation Loss: 0.1902, Validation Accuracy: 0.9427\n",
      "0.0006634204312890623 \n",
      "\n",
      "Epoch 10/100 - Loss: 0.1577, Accuracy: 0.9532\n",
      "Validation Loss: 0.1863, Validation Accuracy: 0.9438\n",
      "0.0006302494097246091 \n",
      "\n",
      "Epoch 11/100 - Loss: 0.1518, Accuracy: 0.9555\n",
      "Validation Loss: 0.1835, Validation Accuracy: 0.9455\n",
      "0.0005987369392383787 \n",
      "\n",
      "Epoch 12/100 - Loss: 0.1467, Accuracy: 0.9570\n",
      "Validation Loss: 0.1809, Validation Accuracy: 0.9476\n",
      "0.0005688000922764596 \n",
      "\n",
      "Epoch 13/100 - Loss: 0.1423, Accuracy: 0.9580\n",
      "Validation Loss: 0.1792, Validation Accuracy: 0.9486\n",
      "0.0005403600876626366 \n",
      "\n",
      "Epoch 14/100 - Loss: 0.1384, Accuracy: 0.9603\n",
      "Validation Loss: 0.1777, Validation Accuracy: 0.9494\n",
      "0.0005133420832795048 \n",
      "\n",
      "Epoch 15/100 - Loss: 0.1351, Accuracy: 0.9616\n",
      "Validation Loss: 0.1766, Validation Accuracy: 0.9511\n",
      "0.00048767497911552955 \n",
      "\n",
      "Epoch 16/100 - Loss: 0.1322, Accuracy: 0.9626\n",
      "Validation Loss: 0.1756, Validation Accuracy: 0.9507\n",
      "0.000463291230159753 \n",
      "\n",
      "Epoch 17/100 - Loss: 0.1296, Accuracy: 0.9637\n",
      "Validation Loss: 0.1751, Validation Accuracy: 0.9511\n",
      "0.00044012666865176535 \n",
      "\n",
      "Epoch 18/100 - Loss: 0.1272, Accuracy: 0.9644\n",
      "Validation Loss: 0.1747, Validation Accuracy: 0.9514\n",
      "0.0004181203352191771 \n",
      "\n",
      "Epoch 19/100 - Loss: 0.1253, Accuracy: 0.9655\n",
      "Validation Loss: 0.1748, Validation Accuracy: 0.9519\n",
      "0.0003972143184582182 \n",
      "\n",
      "Epoch 20/100 - Loss: 0.1233, Accuracy: 0.9660\n",
      "Validation Loss: 0.1751, Validation Accuracy: 0.9523\n",
      "0.00037735360253530727 \n",
      "\n",
      "Epoch 21/100 - Loss: 0.1217, Accuracy: 0.9669\n",
      "Validation Loss: 0.1753, Validation Accuracy: 0.9531\n",
      "0.0003584859224085419 \n",
      "\n",
      "Epoch 22/100 - Loss: 0.1202, Accuracy: 0.9678\n",
      "Validation Loss: 0.1760, Validation Accuracy: 0.9531\n",
      "0.0003405616262881148 \n",
      "\n",
      "Epoch 23/100 - Loss: 0.1189, Accuracy: 0.9684\n",
      "Validation Loss: 0.1765, Validation Accuracy: 0.9533\n",
      "0.000323533544973709 \n",
      "\n",
      "Epoch 24/100 - Loss: 0.1177, Accuracy: 0.9690\n",
      "Validation Loss: 0.1769, Validation Accuracy: 0.9531\n",
      "0.00030735686772502356 \n",
      "\n",
      "Epoch 25/100 - Loss: 0.1164, Accuracy: 0.9698\n",
      "Validation Loss: 0.1773, Validation Accuracy: 0.9535\n",
      "0.0002919890243387724 \n",
      "\n",
      "Epoch 26/100 - Loss: 0.1151, Accuracy: 0.9707\n",
      "Validation Loss: 0.1778, Validation Accuracy: 0.9535\n",
      "0.00027738957312183375 \n",
      "\n",
      "Epoch 27/100 - Loss: 0.1141, Accuracy: 0.9713\n",
      "Validation Loss: 0.1780, Validation Accuracy: 0.9535\n",
      "0.00026352009446574203 \n",
      "\n",
      "Epoch 28/100 - Loss: 0.1131, Accuracy: 0.9718\n",
      "Validation Loss: 0.1784, Validation Accuracy: 0.9535\n",
      "0.00025034408974245495 \n",
      "\n",
      "Epoch 29/100 - Loss: 0.1121, Accuracy: 0.9721\n",
      "Validation Loss: 0.1788, Validation Accuracy: 0.9540\n",
      "0.00023782688525533216 \n",
      "\n",
      "Epoch 30/100 - Loss: 0.1113, Accuracy: 0.9724\n",
      "Validation Loss: 0.1792, Validation Accuracy: 0.9543\n",
      "0.00022593554099256555 \n",
      "\n",
      "Epoch 31/100 - Loss: 0.1105, Accuracy: 0.9727\n",
      "Validation Loss: 0.1797, Validation Accuracy: 0.9551\n",
      "0.00021463876394293727 \n",
      "\n",
      "Epoch 32/100 - Loss: 0.1098, Accuracy: 0.9733\n",
      "Validation Loss: 0.1804, Validation Accuracy: 0.9557\n",
      "0.00020390682574579038 \n",
      "\n",
      "Epoch 33/100 - Loss: 0.1092, Accuracy: 0.9736\n",
      "Validation Loss: 0.1811, Validation Accuracy: 0.9560\n",
      "0.00019371148445850088 \n",
      "\n",
      "Epoch 34/100 - Loss: 0.1086, Accuracy: 0.9739\n",
      "Validation Loss: 0.1817, Validation Accuracy: 0.9557\n",
      "0.00018402591023557584 \n",
      "\n",
      "Epoch 35/100 - Loss: 0.1080, Accuracy: 0.9743\n",
      "Validation Loss: 0.1823, Validation Accuracy: 0.9557\n",
      "0.000174824614723797 \n",
      "\n",
      "Epoch 36/100 - Loss: 0.1074, Accuracy: 0.9746\n",
      "Validation Loss: 0.1827, Validation Accuracy: 0.9557\n",
      "0.00016608338398760718 \n",
      "\n",
      "Epoch 37/100 - Loss: 0.1069, Accuracy: 0.9749\n",
      "Validation Loss: 0.1833, Validation Accuracy: 0.9560\n",
      "0.0001577792147882268 \n",
      "\n",
      "Epoch 38/100 - Loss: 0.1063, Accuracy: 0.9752\n",
      "Validation Loss: 0.1835, Validation Accuracy: 0.9558\n",
      "0.00014989025404881545 \n",
      "\n",
      "Epoch 39/100 - Loss: 0.1057, Accuracy: 0.9753\n",
      "Validation Loss: 0.1838, Validation Accuracy: 0.9555\n",
      "0.00014239574134637466 \n",
      "\n",
      "Epoch 40/100 - Loss: 0.1052, Accuracy: 0.9755\n",
      "Validation Loss: 0.1844, Validation Accuracy: 0.9556\n",
      "0.00013527595427905592 \n",
      "\n",
      "Epoch 41/100 - Loss: 0.1047, Accuracy: 0.9757\n",
      "Validation Loss: 0.1848, Validation Accuracy: 0.9556\n",
      "0.00012851215656510312 \n",
      "\n",
      "Epoch 42/100 - Loss: 0.1041, Accuracy: 0.9759\n",
      "Validation Loss: 0.1852, Validation Accuracy: 0.9558\n",
      "0.00012208654873684796 \n",
      "\n",
      "Epoch 43/100 - Loss: 0.1037, Accuracy: 0.9760\n",
      "Validation Loss: 0.1857, Validation Accuracy: 0.9561\n",
      "0.00011598222130000556 \n",
      "\n",
      "Epoch 44/100 - Loss: 0.1032, Accuracy: 0.9762\n",
      "Validation Loss: 0.1862, Validation Accuracy: 0.9562\n",
      "0.00011018311023500529 \n",
      "\n",
      "Epoch 45/100 - Loss: 0.1028, Accuracy: 0.9764\n",
      "Validation Loss: 0.1867, Validation Accuracy: 0.9564\n",
      "0.00010467395472325501 \n",
      "\n",
      "Epoch 46/100 - Loss: 0.1023, Accuracy: 0.9766\n",
      "Validation Loss: 0.1871, Validation Accuracy: 0.9562\n",
      "9.944025698709225e-05 \n",
      "\n",
      "Epoch 47/100 - Loss: 0.1019, Accuracy: 0.9767\n",
      "Validation Loss: 0.1877, Validation Accuracy: 0.9561\n",
      "9.446824413773763e-05 \n",
      "\n",
      "Epoch 48/100 - Loss: 0.1015, Accuracy: 0.9767\n",
      "Validation Loss: 0.1882, Validation Accuracy: 0.9558\n",
      "8.974483193085076e-05 \n",
      "\n",
      "Epoch 49/100 - Loss: 0.1011, Accuracy: 0.9767\n",
      "Validation Loss: 0.1886, Validation Accuracy: 0.9558\n",
      "8.52575903343082e-05 \n",
      "\n",
      "Epoch 50/100 - Loss: 0.1008, Accuracy: 0.9767\n",
      "Validation Loss: 0.1892, Validation Accuracy: 0.9560\n",
      "8.099471081759279e-05 \n",
      "\n",
      "Epoch 51/100 - Loss: 0.1004, Accuracy: 0.9768\n",
      "Validation Loss: 0.1896, Validation Accuracy: 0.9561\n",
      "7.694497527671315e-05 \n",
      "\n",
      "Epoch 52/100 - Loss: 0.1001, Accuracy: 0.9770\n",
      "Validation Loss: 0.1901, Validation Accuracy: 0.9561\n",
      "7.30977265128775e-05 \n",
      "\n",
      "Epoch 53/100 - Loss: 0.0998, Accuracy: 0.9771\n",
      "Validation Loss: 0.1905, Validation Accuracy: 0.9560\n",
      "6.94428401872336e-05 \n",
      "\n",
      "Epoch 54/100 - Loss: 0.0994, Accuracy: 0.9770\n",
      "Validation Loss: 0.1909, Validation Accuracy: 0.9558\n",
      "6.597069817787194e-05 \n",
      "\n",
      "Epoch 55/100 - Loss: 0.0992, Accuracy: 0.9773\n",
      "Validation Loss: 0.1914, Validation Accuracy: 0.9562\n",
      "6.267216326897833e-05 \n",
      "\n",
      "Epoch 56/100 - Loss: 0.0990, Accuracy: 0.9775\n",
      "Validation Loss: 0.1918, Validation Accuracy: 0.9561\n",
      "5.953855510552941e-05 \n",
      "\n",
      "Epoch 57/100 - Loss: 0.0987, Accuracy: 0.9777\n",
      "Validation Loss: 0.1921, Validation Accuracy: 0.9557\n",
      "5.656162735025293e-05 \n",
      "\n",
      "Epoch 58/100 - Loss: 0.0985, Accuracy: 0.9779\n",
      "Validation Loss: 0.1925, Validation Accuracy: 0.9555\n",
      "5.373354598274029e-05 \n",
      "\n",
      "Epoch 59/100 - Loss: 0.0983, Accuracy: 0.9780\n",
      "Validation Loss: 0.1928, Validation Accuracy: 0.9557\n",
      "5.1046868683603266e-05 \n",
      "\n",
      "Epoch 60/100 - Loss: 0.0981, Accuracy: 0.9781\n",
      "Validation Loss: 0.1931, Validation Accuracy: 0.9555\n",
      "4.8494525249423105e-05 \n",
      "\n",
      "Epoch 61/100 - Loss: 0.0979, Accuracy: 0.9782\n",
      "Validation Loss: 0.1934, Validation Accuracy: 0.9554\n",
      "4.6069798986951947e-05 \n",
      "\n",
      "Epoch 62/100 - Loss: 0.0977, Accuracy: 0.9783\n",
      "Validation Loss: 0.1937, Validation Accuracy: 0.9554\n",
      "4.3766309037604346e-05 \n",
      "\n",
      "Epoch 63/100 - Loss: 0.0976, Accuracy: 0.9784\n",
      "Validation Loss: 0.1940, Validation Accuracy: 0.9550\n",
      "4.157799358572413e-05 \n",
      "\n",
      "Epoch 64/100 - Loss: 0.0974, Accuracy: 0.9784\n",
      "Validation Loss: 0.1943, Validation Accuracy: 0.9546\n",
      "3.949909390643792e-05 \n",
      "\n",
      "Epoch 65/100 - Loss: 0.0973, Accuracy: 0.9784\n",
      "Validation Loss: 0.1946, Validation Accuracy: 0.9548\n",
      "3.752413921111602e-05 \n",
      "\n",
      "Epoch 66/100 - Loss: 0.0971, Accuracy: 0.9785\n",
      "Validation Loss: 0.1949, Validation Accuracy: 0.9545\n",
      "3.564793225056022e-05 \n",
      "\n",
      "Epoch 67/100 - Loss: 0.0970, Accuracy: 0.9785\n",
      "Validation Loss: 0.1952, Validation Accuracy: 0.9545\n",
      "3.386553563803221e-05 \n",
      "\n",
      "Epoch 68/100 - Loss: 0.0969, Accuracy: 0.9786\n",
      "Validation Loss: 0.1955, Validation Accuracy: 0.9546\n",
      "3.217225885613059e-05 \n",
      "\n",
      "Epoch 69/100 - Loss: 0.0967, Accuracy: 0.9786\n",
      "Validation Loss: 0.1957, Validation Accuracy: 0.9546\n",
      "3.0563645913324066e-05 \n",
      "\n",
      "Epoch 70/100 - Loss: 0.0966, Accuracy: 0.9786\n",
      "Validation Loss: 0.1960, Validation Accuracy: 0.9546\n",
      "2.903546361765786e-05 \n",
      "\n",
      "Epoch 71/100 - Loss: 0.0965, Accuracy: 0.9787\n",
      "Validation Loss: 0.1962, Validation Accuracy: 0.9548\n",
      "2.7583690436774966e-05 \n",
      "\n",
      "Epoch 72/100 - Loss: 0.0964, Accuracy: 0.9787\n",
      "Validation Loss: 0.1965, Validation Accuracy: 0.9546\n",
      "2.6204505914936218e-05 \n",
      "\n",
      "Epoch 73/100 - Loss: 0.0963, Accuracy: 0.9789\n",
      "Validation Loss: 0.1967, Validation Accuracy: 0.9548\n",
      "2.4894280619189404e-05 \n",
      "\n",
      "Epoch 74/100 - Loss: 0.0962, Accuracy: 0.9789\n",
      "Validation Loss: 0.1969, Validation Accuracy: 0.9546\n",
      "2.3649566588229933e-05 \n",
      "\n",
      "Epoch 75/100 - Loss: 0.0961, Accuracy: 0.9789\n",
      "Validation Loss: 0.1971, Validation Accuracy: 0.9548\n",
      "2.2467088258818436e-05 \n",
      "\n",
      "Epoch 76/100 - Loss: 0.0960, Accuracy: 0.9791\n",
      "Validation Loss: 0.1973, Validation Accuracy: 0.9549\n",
      "2.134373384587751e-05 \n",
      "\n",
      "Epoch 77/100 - Loss: 0.0960, Accuracy: 0.9791\n",
      "Validation Loss: 0.1975, Validation Accuracy: 0.9549\n",
      "2.0276547153583635e-05 \n",
      "\n",
      "Epoch 78/100 - Loss: 0.0959, Accuracy: 0.9790\n",
      "Validation Loss: 0.1977, Validation Accuracy: 0.9550\n",
      "1.9262719795904453e-05 \n",
      "\n",
      "Epoch 79/100 - Loss: 0.0958, Accuracy: 0.9790\n",
      "Validation Loss: 0.1979, Validation Accuracy: 0.9550\n",
      "1.8299583806109228e-05 \n",
      "\n",
      "Epoch 80/100 - Loss: 0.0957, Accuracy: 0.9790\n",
      "Validation Loss: 0.1980, Validation Accuracy: 0.9550\n",
      "1.7384604615803768e-05 \n",
      "\n",
      "Epoch 81/100 - Loss: 0.0956, Accuracy: 0.9790\n",
      "Validation Loss: 0.1982, Validation Accuracy: 0.9549\n",
      "1.651537438501358e-05 \n",
      "\n",
      "Epoch 82/100 - Loss: 0.0956, Accuracy: 0.9790\n",
      "Validation Loss: 0.1984, Validation Accuracy: 0.9549\n",
      "1.56896056657629e-05 \n",
      "\n",
      "Epoch 83/100 - Loss: 0.0955, Accuracy: 0.9790\n",
      "Validation Loss: 0.1985, Validation Accuracy: 0.9549\n",
      "1.4905125382474753e-05 \n",
      "\n",
      "Epoch 84/100 - Loss: 0.0954, Accuracy: 0.9790\n",
      "Validation Loss: 0.1987, Validation Accuracy: 0.9548\n",
      "1.4159869113351015e-05 \n",
      "\n",
      "Epoch 85/100 - Loss: 0.0954, Accuracy: 0.9790\n",
      "Validation Loss: 0.1988, Validation Accuracy: 0.9548\n",
      "1.3451875657683464e-05 \n",
      "\n",
      "Epoch 86/100 - Loss: 0.0953, Accuracy: 0.9791\n",
      "Validation Loss: 0.1989, Validation Accuracy: 0.9549\n",
      "1.2779281874799288e-05 \n",
      "\n",
      "Epoch 87/100 - Loss: 0.0952, Accuracy: 0.9790\n",
      "Validation Loss: 0.1991, Validation Accuracy: 0.9550\n",
      "1.2140317781059324e-05 \n",
      "\n",
      "Epoch 88/100 - Loss: 0.0952, Accuracy: 0.9790\n",
      "Validation Loss: 0.1992, Validation Accuracy: 0.9550\n",
      "1.1533301892006358e-05 \n",
      "\n",
      "Epoch 89/100 - Loss: 0.0951, Accuracy: 0.9791\n",
      "Validation Loss: 0.1993, Validation Accuracy: 0.9551\n",
      "1.095663679740604e-05 \n",
      "\n",
      "Epoch 90/100 - Loss: 0.0951, Accuracy: 0.9791\n",
      "Validation Loss: 0.1994, Validation Accuracy: 0.9551\n",
      "1.0408804957535738e-05 \n",
      "\n",
      "Epoch 91/100 - Loss: 0.0950, Accuracy: 0.9792\n",
      "Validation Loss: 0.1995, Validation Accuracy: 0.9551\n",
      "9.88836470965895e-06 \n",
      "\n",
      "Epoch 92/100 - Loss: 0.0950, Accuracy: 0.9792\n",
      "Validation Loss: 0.1996, Validation Accuracy: 0.9551\n",
      "9.393946474176e-06 \n",
      "\n",
      "Epoch 93/100 - Loss: 0.0949, Accuracy: 0.9793\n",
      "Validation Loss: 0.1997, Validation Accuracy: 0.9551\n",
      "8.924249150467201e-06 \n",
      "\n",
      "Epoch 94/100 - Loss: 0.0949, Accuracy: 0.9793\n",
      "Validation Loss: 0.1998, Validation Accuracy: 0.9552\n",
      "8.478036692943842e-06 \n",
      "\n",
      "Epoch 95/100 - Loss: 0.0948, Accuracy: 0.9793\n",
      "Validation Loss: 0.1999, Validation Accuracy: 0.9554\n",
      "8.054134858296648e-06 \n",
      "\n",
      "Epoch 96/100 - Loss: 0.0948, Accuracy: 0.9793\n",
      "Validation Loss: 0.2000, Validation Accuracy: 0.9552\n",
      "7.651428115381816e-06 \n",
      "\n",
      "Epoch 97/100 - Loss: 0.0948, Accuracy: 0.9794\n",
      "Validation Loss: 0.2001, Validation Accuracy: 0.9552\n",
      "7.2688567096127245e-06 \n",
      "\n",
      "Epoch 98/100 - Loss: 0.0947, Accuracy: 0.9794\n",
      "Validation Loss: 0.2001, Validation Accuracy: 0.9551\n",
      "6.905413874132088e-06 \n",
      "\n",
      "Epoch 99/100 - Loss: 0.0947, Accuracy: 0.9794\n",
      "Validation Loss: 0.2002, Validation Accuracy: 0.9552\n",
      "6.560143180425483e-06 \n",
      "\n",
      "Epoch 100/100 - Loss: 0.0946, Accuracy: 0.9794\n",
      "Validation Loss: 0.2003, Validation Accuracy: 0.9551\n",
      "6.2321360214042085e-06 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize neural network parameters\n",
    "input_size = 245\n",
    "hidden_layer_1_size = 512\n",
    "hidden_layer_2_size = 128\n",
    "output_size = 10\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = np.random.randn(input_size, hidden_layer_1_size) * 0.01\n",
    "b1 = np.zeros((1, hidden_layer_1_size))\n",
    "\n",
    "w2 = np.random.randn(hidden_layer_1_size, hidden_layer_2_size) * 0.01\n",
    "b2 = np.zeros((1, hidden_layer_2_size))\n",
    "\n",
    "w3 = np.random.randn(hidden_layer_2_size, output_size) * 0.01\n",
    "b3 = np.zeros((1, output_size))\n",
    "\n",
    "# Adam optimizer state variables (momentum, velocity)\n",
    "m_w1, v_w1 = np.zeros_like(w1), np.zeros_like(w1)\n",
    "m_b1, v_b1 = np.zeros_like(b1), np.zeros_like(b1)\n",
    "m_w2, v_w2 = np.zeros_like(w2), np.zeros_like(w2)\n",
    "m_b2, v_b2 = np.zeros_like(b2), np.zeros_like(b2)\n",
    "m_w3, v_w3 = np.zeros_like(w3), np.zeros_like(w3)\n",
    "m_b3, v_b3 = np.zeros_like(b3), np.zeros_like(b3)\n",
    "\n",
    "beta1, beta2 = 0.9, 0.999  # Adam parameters\n",
    "epsilon = 1e-8  # For numerical stability\n",
    "\n",
    "# Learning rate decay\n",
    "lr_scheduler = lambda epoch: learning_rate * max(0.95 ** epoch, 1e-6)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(features_array, one_hot_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    num_batches = int(x_train.shape[0] / batch_size)\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        # Get the current batch\n",
    "        start = batch * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = x_train[start:end]\n",
    "        y_batch = y_train[start:end]\n",
    "        \n",
    "        # Forward pass\n",
    "        z1 = np.dot(X_batch, w1) + b1\n",
    "        a1 = relu(z1)\n",
    "        \n",
    "        z2 = np.dot(a1, w2) + b2\n",
    "        a2 = relu(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, w3) + b3\n",
    "        y_pred = softmax(z3)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = cross_entropy_loss(y_batch, y_pred)\n",
    "        epoch_loss += loss\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = np.mean(np.argmax(y_pred, axis=1) == np.argmax(y_batch, axis=1))\n",
    "        epoch_accuracy += accuracy\n",
    "\n",
    "        # Backpropagation\n",
    "        # Output layer gradients\n",
    "        dz3 = y_pred - y_batch  # Gradient of loss w.r.t output layer\n",
    "        dw3 = np.dot(a2.T, dz3) / batch_size\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 2 gradients\n",
    "        dz2 = np.dot(dz3, w3.T) * relu_derivative(a2)\n",
    "        dw2 = np.dot(a1.T, dz2) / batch_size\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Hidden layer 1 gradients\n",
    "        dz1 = np.dot(dz2, w2.T) * relu_derivative(a1)\n",
    "        dw1 = np.dot(X_batch.T, dz1) / batch_size\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / batch_size\n",
    "        \n",
    "        # Adam optimizer parameter update\n",
    "        for param, grad, m, v, lr in zip([w1, w2, w3], [dw1, dw2, dw3], [m_w1, m_w2, m_w3], [v_w1, v_w2, v_w3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "        for param, grad, m, v, lr in zip([b1, b2, b3], [db1, db2, db3], [m_b1, m_b2, m_b3], [v_b1, v_b2, v_b3], [lr_scheduler(epoch)] * 3):\n",
    "            m = beta1 * m + (1 - beta1) * grad\n",
    "            v = beta2 * v + (1 - beta2) * np.square(grad)\n",
    "            m_hat = m / (1 - beta1 ** (epoch + 1))  # Bias correction\n",
    "            v_hat = v / (1 - beta2 ** (epoch + 1))  # Bias correction\n",
    "            param -= lr * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "\n",
    "    # Epoch metrics\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Loss: {epoch_loss/num_batches:.4f}, Accuracy: {epoch_accuracy/num_batches:.4f}\")\n",
    "    \n",
    "    # Validation loss and accuracy after every epoch\n",
    "    z1_val = np.dot(x_test, w1) + b1\n",
    "    a1_val = relu(z1_val)\n",
    "    z2_val = np.dot(a1_val, w2) + b2\n",
    "    a2_val = relu(z2_val)\n",
    "    z3_val = np.dot(a2_val, w3) + b3\n",
    "    y_pred_val = softmax(z3_val)\n",
    "    \n",
    "    val_loss = cross_entropy_loss(y_test, y_pred_val)\n",
    "    val_accuracy = np.mean(np.argmax(y_pred_val, axis=1) == np.argmax(y_test, axis=1))\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(lr,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63349c-b803-499d-ad3b-12739764ceb8",
   "metadata": {},
   "source": [
    "# Attach image to predict"
   ]
  },
  {
   "attachments": {
    "6878b302-6f1b-4cce-bc73-a88eb52e5985.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/2wBDAAYEBQYFBAYGBQYHBwYIChAKCgkJChQODwwQFxQYGBcUFhYaHSUfGhsjHBYWICwgIyYnKSopGR8tMC0oMCUoKSj/2wBDAQcHBwoIChMKChMoGhYaKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCj/wAARCAA4ADwDASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6ZApXISJnbgKMk0q8Cs3xNeCx0W4lY4+XNKK1Ao6Z4p0/UtWl061kLXES7mG09M4rcPXmvD/gWraj4t1jUmyVKsgP0evbzndRNWAWnUwGpByahACinZNNPHSl3UwMnxKL9bNRpe3zMjO4E9653x1Het4Rdbpl80oc7ciu8XtiuU+Jb7dAc9gpzVw3Ezl/gRpX2LQbmYjazzSAk/UV6FPd2URxNdwq3oXxXieg+Kb+60T+ytBjIlMzbpASpGeK2LH4ea1e4uL7VLne3JXeDitJpPcEepxXtpJxHcxMfZqs7toGOc15LrPhXV9DtzcWt3NJs5ILV1Xw68Tp4i00K3FzFneD9cVLgraDOzBAp2RUWPWnVkBKpwKx/FmmtqukTQKOSpFa6tTsj0ppgeSfCW3sdE1W7025VUvMtIM9wW4r1xmYHjkVxXjHwZHqswvrBzBfpyGBIHHTIFc9DrHjDSv9HljW4ROAyw/41o/eA9SvVjks5BPjbtOc15B8L7E23jPV3s/+PN0UKR0zu5q/Nf8AirXR9nZBBE3UmLH8q7PwroUehaYIVw0pyWbuc80/hQGy/BozSMM0YrFoCSnDgUUUAG7byf4uKa6RZ+aNT9RRRTuAbI1+4ij6CmnrRRRcBD1ooooA/9k="
    },
    "b24061ed-10ac-4eed-a1d9-f3b2cb224ae3.jpg": {
     "image/jpeg": "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"
    },
    "e230ce8d-ebf8-466b-893c-b785a5e1e7fe.jpg": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgICAgMCAgIDAwMDBAYEBAQEBAgGBgUGCQgKCgkICQkKDA8MCgsOCwkJDRENDg8QEBEQCgwSExIQEw8QEBD/2wBDAQMDAwQDBAgEBAgQCwkLEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBD/wAARCAAcABwDASIAAhEBAxEB/8QAGQAAAwADAAAAAAAAAAAAAAAAAwYHBQgJ/8QALRAAAQMCBQIEBgMAAAAAAAAAAgEDBAURAAYHEiEVIhMxQYEIFiNRYXEkYpH/xAAXAQEAAwAAAAAAAAAAAAAAAAABAAID/8QAHBEAAgIDAQEAAAAAAAAAAAAAAAECEQMhMRIT/9oADAMBAAIRAxEAPwDmYw07NlswI4Kbj7otAKJdVVVt5euH/W/TWHpJqTUMgRKmc4qXHirIcMdpA+4wDjjdv6kaj7Yx2l9ZjfNWWKN0SI5K67HfWaW7eo7k7LXtb2xX9VtI8x5x1KzhqNn3ONCydFq9cmuwxq7v15LaPEIqIcdtkSy/bBLJ4ypPg1o1xcWy3LiyYGQ89wbvzh+z/pVVckQafW+oxKxRKohpDqULlkzFe4FX0JLotvzhENtUXtNf9xvCaybiASkVWVQK1DrkJP5EGQEhr7bhW6YtupeTa/8AEHmGRq3kitR8wSKygvVKnSZwNzKfJUU8RtAcJCJtCvtUbpa2IU4tx59VtgsZ2RGLxIsp9gjQrq04oeS2Ty/WKThbtdG9UWHMQVLTLRA9J8zzYz1VrNbGsBT25APdNBtvZuVQVUEnL8p52FL4kaoPH6TAkddeLxX3TdcNLkZkpEvuuCLxiYo/NAf/2Q=="
    }
   },
   "cell_type": "markdown",
   "id": "79b78481-6567-46f3-9037-2fa443387268",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "\n",
    "1. Take a photo of the number.\n",
    "    \n",
    "2. Crop the photo such that there is not a lot of empty space between the number and the borders of the image.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840.jpg](attachment:6878b302-6f1b-4cce-bc73-a88eb52e5985.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "4. Resize the image to 28x28 and convert it to a .jpg file.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![Screenshot 2024-11-30 162840 (2).jpg](attachment:b24061ed-10ac-4eed-a1d9-f3b2cb224ae3.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "6. Invert the image's colours.\n",
    "\n",
    "<div style=\"margin-left: 40px;\">\n",
    "\n",
    "![image.jpg](attachment:e230ce8d-ebf8-466b-893c-b785a5e1e7fe.jpg)\n",
    "\n",
    "</div>\n",
    "    \n",
    "8. Copy the path of the image file into the cell and hit run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c112a014-d303-4771-8e80-d2c176e84c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 6\n"
     ]
    }
   ],
   "source": [
    "def predict(x_in):\n",
    "    # Forward pass through the network\n",
    "    z1 = np.dot(x_in, w1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, w2) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(a2, w3) + b3\n",
    "    y_pred = softmax(z3)\n",
    "\n",
    "    # Get the index of the maximum value (predicted class)\n",
    "    return np.argmax(y_pred)\n",
    "\n",
    "# Load the image\n",
    "image_path = r\"C:\\Users\\vijay\\Documents\\test_images\\Screenshot 2024-11-30 162908.jpg\" # Paste image file here.\n",
    "image = Image.open(image_path).convert('L')  # Convert to grayscale since not all pictures are black and white like the dataset.\n",
    "image = image.point(lambda p: 255 if p >= 128 else 0, '1')\n",
    "image = np.array(image) / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Extract features (e.g., flatten the image)\n",
    "x_in = extract_features(image)\n",
    "\n",
    "# Predict the class of the input image\n",
    "predicted_class = predict(x_in)\n",
    "\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
